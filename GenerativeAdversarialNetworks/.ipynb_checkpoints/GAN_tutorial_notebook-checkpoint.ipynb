{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by [Edoardo Cetin](edoardo.cetin@kcl.ac.uk)\n",
    "\n",
    "For [The AI Core](https://theaicore.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Aladoro/tutorials/blob/master/GenerativeAdversarialNetworks/GAN_tutorial_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OejFa_seHkN9"
   },
   "source": [
    "# Generative Adversarial Networks - Tutorial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-gn88JZHkX5"
   },
   "source": [
    "Purpose: we want to obtain some *distributional representation* of some given set data, in the form of sampler.\n",
    "\n",
    "**Input:** The dataset we want to represent.\n",
    "\n",
    "**Output:** A  (neural network) model (*the generator*) which can generate new samples based on the structurally consistencies of our dataset samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KE55E0z5IrOD"
   },
   "source": [
    "Current capabilities of GANs:\n",
    "\n",
    "https://www.youtube.com/watch?v=6E1_dgYlifc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rs7GZvX9I2aj"
   },
   "source": [
    "## A step backwards, **the original paper**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5S57xgsDle4"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=1VQtjYYJ95V7flzsik368_Nu25lrEU_XA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqmoLh_dFj0B"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=1RgfNOgDF-6qLNBC_lJcNt3wsxu0SX7Sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raBGxvZaGWI2"
   },
   "source": [
    "## Key equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFBt8FF8FkGZ"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=11Tc_OAZEbvhFag7fh0SAADTzxXY_RjV8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXZyXBT8Je9l"
   },
   "source": [
    "### **Main concept of GANs:**\n",
    "\n",
    "We train 2 Networks:\n",
    "\n",
    "\n",
    "1.   A Generator **G**:\n",
    "\n",
    "\n",
    "*   Takes as input random variables **z** from a fixed noise distribution **P(z)**\n",
    "*   Outputs generated samples **G(z)**, which we would want to be close to our real data distribution **p_*data(x)***(from our dataset) \n",
    "\n",
    "---\n",
    "\n",
    "2.   A Discriminator **D**:\n",
    "\n",
    "\n",
    "*   Takes as input samples either from our dataset, **x_*real***, or generated from **G**, **x_*gen***\n",
    "*   Outputs a probability for each sample **D(x)**, ideally proportional to how likely its input **x** came from our dataset\n",
    "\n",
    "\n",
    "> **D** is trained as a classifier, to assign a high score to real sample and a low score to generated sample through the cross-entropy loss.\n",
    "\n",
    "\n",
    "> **G** is trained to adversarially fool the discriminator, to obtain higher scores.\n",
    "\n",
    "Hence, 2 Neural Networks are pitted against each other, a classifier (the discriminator) and a generator. \n",
    "The classifier improves by learning to classify the real from the generated samples, the generator by learning how to bring its samples closer to the real samples (in order to fool the classifier).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcGsN6RydmrS"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=1w4DPCzn1AfUsbp4x_AeWAmIP2hgGPYpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VY8kcxMeOB4h"
   },
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ql11AwtEOP2p"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=1HppTTWvwQHa_9BV1zy42fjdT-RVH5s6d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9N9Bfz13Wnp"
   },
   "source": [
    "Samples produced by the original GAN work were good for the time (2014), but nowhere near comparable the ones produced by today's model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOqS5vXJdWhl"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=12ydeEfQuGlJ5DlVb0j_v0anMy9W3-f4r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJDIFAsq3Xa_"
   },
   "source": [
    "## Further major breakthrough: **DCGAN** (2015)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DfnYQ76R69tR"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=1d2CMtD48BP-7SjmCli4I8D0V45cgArzc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFjO-xGt3X5Q"
   },
   "source": [
    "This work introduced and/or popularized many practices that became standards in GAN training for many further works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfrPiNnu9SQI"
   },
   "source": [
    "\n",
    "\n",
    "> 1. **Discriminator** and **Generator** architectural practices:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usvV7w_Z9djS"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=1_a4uL1nSqLqs8Rnp7da4r_8eZ5JORdDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ADK9Ljdl94By"
   },
   "source": [
    "\n",
    "\n",
    "> 2. Non-saturating **Generator** loss:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lC4F5bUd9-2n"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=1Wm2MAlU-2VK5VV6bGyihB9_aWOEhroWr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Sa8B781err8"
   },
   "source": [
    "### Different **Generator** objectives visualizations:\n",
    "\n",
    "Minimax = original proposed objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNYVP0Pmefto"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=1oJGt5vWmSrVIy4LEX1sjw-WPXiS-3HMm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pc2zHRQXiEGa"
   },
   "source": [
    "### Comparison on *MNIST* dataset:\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1uA4RsuTpzy_6no_0iNYhS0KbzBRCSFWJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJaiPCr-OvNB"
   },
   "source": [
    "## Implementation on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbfFczdrQxgB"
   },
   "source": [
    "### Train utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywlNRZxnPF5D"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fFmw74BQquu"
   },
   "outputs": [],
   "source": [
    "def display_samples(samples, images_per_row=5):\n",
    "  float_samples = samples\n",
    "  number_samples = samples.shape[0]\n",
    "  grid_rows = int(np.ceil(number_samples/images_per_row))\n",
    "  plt.figure(figsize=(images_per_row, grid_rows))\n",
    "  for i in range(number_samples):\n",
    "    plt.subplot(grid_rows, images_per_row, i+1)\n",
    "    plt.imshow(float_samples[i])\n",
    "    plt.axis('off')\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(dataset, batch_size):\n",
    "  indices = np.random.randint(dataset.shape[0], size=batch_size)\n",
    "  return dataset[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, batch_size, updates_per_epoch=1000,\n",
    "                disc_steps_per_update=1, gen_steps_per_update=1, log_every=100,\n",
    "                lr_to_decay=None):\n",
    "  gen_losses = []\n",
    "  disc_losses = []\n",
    "  for i in range(updates_per_epoch):\n",
    "    for step in range(disc_steps_per_update):\n",
    "      batch = get_random_batch(dataset, batch_size)\n",
    "      disc_loss = model.step_discriminator(batch, batch_size)\n",
    "      disc_losses.append(disc_loss)\n",
    "    for step in range(gen_steps_per_update):\n",
    "      gen_loss = model.step_generator(batch_size)\n",
    "      gen_losses.append(gen_loss)\n",
    "    if lr_to_decay is not None:\n",
    "      for lr in lr_to_decay:\n",
    "        lr.decay_lr()\n",
    "    if i % log_every == 0:\n",
    "      print('Update {}/{}'.format(i+1, updates_per_epoch))\n",
    "      print('Discriminator loss: {}; Generator loss: {}'.format(\n",
    "          disc_loss, gen_loss\n",
    "      ))\n",
    "      if lr_to_decay is not None:\n",
    "        for j, lr in enumerate(lr_to_decay):\n",
    "          print('Current learning rate {}: {}'.format(j, lr()))\n",
    "  return gen_losses, disc_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(model, data, batch_size, epochs, samples_to_display=0, \n",
    "              disc_steps_per_update=1, gen_steps_per_update=1, \n",
    "              log_every=100, tanh_outputs=False, lr_to_decay=None):\n",
    "  _ = model(1)\n",
    "  model.summary()\n",
    "  gen_losses = []\n",
    "  disc_losses = []\n",
    "  size = data.shape[0]\n",
    "  updates_per_epoch = size // batch_size\n",
    "  for e in range(epochs):\n",
    "    print('Training epoch {}/{}'.format(e+1, epochs))\n",
    "    e_gen_losses, e_disc_losses = train_epoch(\n",
    "        model, data, batch_size, updates_per_epoch, disc_steps_per_update, \n",
    "        gen_steps_per_update, log_every, lr_to_decay)\n",
    "    \n",
    "    gen_losses += e_gen_losses\n",
    "    disc_losses += e_disc_losses\n",
    "\n",
    "    if samples_to_display > 0:\n",
    "      samples = model.generate(samples_to_display)\n",
    "      if samples.shape[-1] == 1:\n",
    "        samples = np.reshape(samples, samples.shape[:-1])\n",
    "      if tanh_outputs:\n",
    "        samples = (samples + 1)/2\n",
    "      display_samples(samples)\n",
    "\n",
    "  return gen_losses, disc_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uStAN18qivbW"
   },
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aawwdXwTOqOB"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfl = tf.keras.layers\n",
    "tfo = tf.keras.optimizers\n",
    "tfm = tf.keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKBB3qdEkuzG"
   },
   "outputs": [],
   "source": [
    "def run_layers(inputs, layers):\n",
    "  out = inputs\n",
    "  for layer in layers:\n",
    "    out = layer(out)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tfl.Layer):\n",
    "  def __init__(self, layers):\n",
    "    super(Generator, self).__init__()\n",
    "    self._gen_layers = layers\n",
    "  def call(self, inputs):\n",
    "    return run_layers(inputs, self._gen_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tfl.Layer):\n",
    "  def __init__(self, layers):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self._disc_layers = layers\n",
    "  def call(self, inputs):\n",
    "    return run_layers(inputs, self._disc_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGAN(tfm.Model):\n",
    "  def __init__(self, noise_dims, generator, discriminator, non_saturating=False,\n",
    "               gen_optimizer=tfo.Adam(1e-3),\n",
    "               disc_optimizer=tfo.Adam(1e-3)):\n",
    "    super(SimpleGAN, self).__init__()\n",
    "    self._input_distribution = tfp.distributions.MultivariateNormalDiag(\n",
    "        loc=tf.zeros([noise_dims]), scale_diag=tf.ones([noise_dims]))\n",
    "    self._gen = generator\n",
    "    self._disc = discriminator\n",
    "    self._gen_opt = gen_optimizer\n",
    "    self._disc_opt = disc_optimizer\n",
    "    self.step_discriminator = self._make_discriminator_trainining_op()\n",
    "    self.step_generator = self._make_generator_training_op(non_saturating)\n",
    "\n",
    "  def call(self, batch_size):\n",
    "    out = {'gen': self.generate(batch_size)}\n",
    "    out['dis'] = self._disc(out['gen'])\n",
    "    return out\n",
    "\n",
    "  def generate(self, batch_size):\n",
    "    # batch_size: number of datapoints we want to generate\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def discriminator_loss(self, real_data, fake_data_n):\n",
    "    # real_data: batch of datapoints, from the dataset we want to represent\n",
    "    # fake_data_n: number of samples to generate and evaluate\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def generator_loss(self, fake_data_n):\n",
    "    # fake_data_n: number of samples to generate and evaluate\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def non_saturating_generator_loss(self, fake_data_n):\n",
    "    # fake_data_n: number of samples to generate and evaluate\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def _make_discriminator_trainining_op(self,):\n",
    "    def train(real_data, fake_data_n):\n",
    "      with tf.GradientTape() as tape:\n",
    "        disc_loss = self.discriminator_loss(real_data, fake_data_n)\n",
    "      gradients = tape.gradient(disc_loss, self._disc.trainable_weights)\n",
    "      self._disc_opt.apply_gradients(zip(gradients, \n",
    "                                         self._disc.trainable_weights))\n",
    "      return disc_loss\n",
    "    return tf.function(train)\n",
    "  \n",
    "  def _make_generator_training_op(self, non_saturating=False):\n",
    "    if non_saturating:\n",
    "      generator_loss = self.non_saturating_generator_loss\n",
    "    else:\n",
    "      generator_loss = self.generator_loss\n",
    "    def train(fake_data_n):\n",
    "      with tf.GradientTape() as tape:\n",
    "        gen_loss = generator_loss(fake_data_n)\n",
    "      gradients = tape.gradient(gen_loss, self._gen.trainable_weights)\n",
    "      self._gen_opt.apply_gradients(zip(gradients, self._gen.trainable_weights))\n",
    "      return gen_loss\n",
    "    return tf.function(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pl0xCoeQrv6"
   },
   "source": [
    "## Training on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5aNw_p8g8gKE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (_, _) = mnist.load_data()\n",
    "x_train = np.expand_dims(x_train.astype('float32')/255, axis=-1)\n",
    "\n",
    "random_data = np.squeeze(x_train[np.random.randint(x_train.shape[0], size=10)])\n",
    "display_samples(random_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaM9m0FmttGt"
   },
   "source": [
    "#### Custom models based on DCGAN practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYpVFV67tsM-"
   },
   "outputs": [],
   "source": [
    "class MnistGenerator(Generator):\n",
    "  def __init__(self,):\n",
    "    layers = [\n",
    "          tfl.Dense(1024, input_shape=(74,)),\n",
    "          tfl.BatchNormalization(axis=-1),\n",
    "          tfl.Activation('relu'),\n",
    "          tfl.Dense(7*7*128),\n",
    "          tfl.BatchNormalization(axis=-1),\n",
    "          tfl.Activation('relu'),\n",
    "          tfl.Reshape((7, 7, 128)),\n",
    "          tfl.Conv2DTranspose(64, 4, strides=(2, 2), padding='same'),\n",
    "          tfl.BatchNormalization(axis=-1),\n",
    "          tfl.Activation('relu'),\n",
    "          tfl.Conv2DTranspose(1, 4, strides=(2, 2), padding='same'),\n",
    "          tfl.Activation('sigmoid'),\n",
    "      ]\n",
    "    super(MnistGenerator, self).__init__(layers=layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDiscriminator(Discriminator):\n",
    "  def __init__(self,):\n",
    "    layers = [\n",
    "          tfl.Conv2D(64, 4, strides=(2, 2), padding='same'),\n",
    "          tfl.LeakyReLU(),\n",
    "          tfl.Conv2D(128, 4, strides=(2, 2), padding='same'),\n",
    "          tfl.BatchNormalization(axis=-1),\n",
    "          tfl.LeakyReLU(),\n",
    "          tfl.Reshape((7*7*128,)),\n",
    "          tfl.Dense(1024),\n",
    "          tfl.BatchNormalization(axis=-1),\n",
    "          tfl.LeakyReLU(),\n",
    "          tfl.Dense(1, activation='sigmoid')\n",
    "      ]\n",
    "    super(MnistDiscriminator, self).__init__(layers=layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9xSLmrDSkFY"
   },
   "outputs": [],
   "source": [
    "generator = MnistGenerator()\n",
    "discriminator = MnistDiscriminator()\n",
    "\n",
    "gan = SimpleGAN(noise_dims=128,\n",
    "                generator=generator,\n",
    "                discriminator=discriminator,\n",
    "                non_saturating=True,\n",
    "                gen_optimizer=tfo.Adam(2e-4),\n",
    "                disc_optimizer=tfo.Adam(2e-4))\n",
    "\n",
    "tf.keras.backend.set_learning_phase(\n",
    "  1\n",
    ")\n",
    "gen_losses, disc_losses = train_gan(gan, x_train,\n",
    "                                    batch_size=128,\n",
    "                                    epochs=20, \n",
    "                                    samples_to_display=10, \n",
    "                                    disc_steps_per_update=1, \n",
    "                                    gen_steps_per_update=1, \n",
    "                                    log_every=200, \n",
    "                                    tanh_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMDvOff-mXmw"
   },
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3eTsry8maxt"
   },
   "outputs": [],
   "source": [
    "iterations_range = np.arange(len(gen_losses))\n",
    "plt.plot(iterations_range, gen_losses, label='generator loss')\n",
    "plt.plot(iterations_range, disc_losses, label='discriminator loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEtqgFKVojNk"
   },
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwJlfVaFnJDL"
   },
   "source": [
    "made by Edoardo Cetin (edoardo.cetin@kcl.ac.uk), utilizing figures from:\n",
    "\n",
    "*Generative Adversarial Nets (Goodfellow et al. 2014)*\n",
    "\n",
    "*Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford et al. 2016)*\n",
    "\n",
    "*NIPS 2016 Tutorial: Generative Adversarial Networks (Goodfellow et al. 2017)*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNCybD8ap9/J6u3LMqvDBaN",
   "collapsed_sections": [
    "Rs7GZvX9I2aj",
    "raBGxvZaGWI2",
    "VY8kcxMeOB4h",
    "IJDIFAsq3Xa_",
    "6Sa8B781err8",
    "IbfFczdrQxgB",
    "4pl0xCoeQrv6"
   ],
   "include_colab_link": true,
   "name": "GAN-tutorial-notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
