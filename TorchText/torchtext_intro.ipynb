{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext tutorial\n",
    "\n",
    "\n",
    "Made by [Andrea Sottana](http://github.com/AndreaSottana) and adapted from the following [tutorial](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)\n",
    "\n",
    "This notebook shows how you can use your own custom dataset starting from the raw form (for example a .csv file) with the `torch` library, to easily do some cleaning and preprocessing (such as tokenization), build a vocabulary, and ultimately create an `Iterator` or `BucketIterator` object that you can feed it into a neural network built using the `torch` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset, Iterator, BucketIterator\n",
    "from torchtext import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=20)  # shows info level loggings (we'll use this to print the loss during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: multi-class classification, single label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data `Field` specifies how you want a certain field to be processed, for example whether you want to tokenize it, lower case the text etc. In our case we have one text field, and one label field for the sentiment analysis, which we represent with an integer, where 0 is negative, 1 is neutral and 2 is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize='spacy', lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>numerical_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>No surprises .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels  \\\n",
       "0     The Rock is destined to be the 21st Century 's...  positive   \n",
       "1     The gorgeously elaborate continuation of `` Th...  positive   \n",
       "2     Singer\\/composer Bryan Adams contributes a sle...  positive   \n",
       "3     You 'd think by now America would have had eno...   neutral   \n",
       "4                  Yet the act is still charming here .  positive   \n",
       "...                                                 ...       ...   \n",
       "8539                                    A real snooze .  negative   \n",
       "8540                                     No surprises .  negative   \n",
       "8541  We 've seen the hippie-turned-yuppie plot befo...  positive   \n",
       "8542  Her fans walked out muttering words like `` ho...  negative   \n",
       "8543                                In this case zero .  negative   \n",
       "\n",
       "      numerical_labels  \n",
       "0                    2  \n",
       "1                    2  \n",
       "2                    2  \n",
       "3                    1  \n",
       "4                    2  \n",
       "...                ...  \n",
       "8539                 0  \n",
       "8540                 0  \n",
       "8541                 2  \n",
       "8542                 0  \n",
       "8543                 0  \n",
       "\n",
       "[8544 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember to source the .envrc file in the terminal before launching this notebook to \n",
    "# ensure it can use the environment variables correctly.\n",
    "\n",
    "folder = os.path.join(os.getenv('DATA_DIR'), 'movie_review_dataset')\n",
    "train_dataset = pd.read_csv(os.path.join(folder, 'train_dataset.csv'))\n",
    "valid_dataset = pd.read_csv(os.path.join(folder, 'valid_dataset.csv'))\n",
    "test_dataset = pd.read_csv(os.path.join(folder, 'test_dataset.csv'))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells the field what data to work on. Note that the fields we pass in must be in the same order as the columns in our csv file. For the columns we don't use, we pass in a tuple where the field element is None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', <torchtext.data.field.Field at 0x136a4be10>),\n",
       " ('labels', None),\n",
       " ('numerical_labels', <torchtext.data.field.Field at 0x1383d40b8>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [(\"text\", TEXT), (\"labels\", None), (\"numerical_labels\", LABEL)]\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TabularDataset` is one of the built-in Datasets in torchtext that handle common data formats; this one is good for .csv files. Other datasets, such as `LanguageModelingDataset` or `TranslationDataset`, are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TabularDataset.splits(\n",
    "    path = folder,\n",
    "    train = 'train_dataset.csv',\n",
    "    validation = 'valid_dataset.csv',\n",
    "    test = 'test_dataset.csv',\n",
    "    format = 'csv',\n",
    "    skip_header = True,\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x1401d1320> \n",
      "\n",
      "{'text': ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '`', '`', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', '-', 'claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], 'numerical_labels': '2'} \n",
      "\n",
      "dict_keys(['text', 'numerical_labels']) \n",
      "\n",
      "['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '`', '`', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', '-', 'claud', 'van', 'damme', 'or', 'steven', 'segal', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train[0], '\\n')\n",
    "\n",
    "# This is an Example object. The Example object bundles the attributes of a single data \n",
    "# point together. We also see that the text has already been tokenized for us, but has \n",
    "# not yet been converted to integers. \n",
    "\n",
    "print(train[0].__dict__, '\\n')\n",
    "print(train[0].__dict__.keys(), '\\n')\n",
    "print(train[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is constructing the mapping from words to ids. The vocabulary is normally built on the training set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext has its own class called Vocab for handling the vocabulary. The Vocab class holds a mapping from word to id in its `stoi` attribute and a reverse mapping in its `itos` attribute. Words that are not included in the vocabulary will be converted into `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[12])\n",
    "print(TEXT.vocab.stoi['it'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create an Iterator to pass the data to our model.  \n",
    "\n",
    "In torchvision and PyTorch, the processing and batching of data is handled by DataLoaders. For some reason, torchtext has renamed the objects that do the exact same thing to Iterators. The basic functionality is the same, but Iterators, as we will see, have some convenient functionality that is unique to NLP.  \n",
    "\n",
    "We also need to tell the BucketIterator what attribute you want to bucket the data on. In our case, we want to bucket based on the lengths of the comment_text field, so we pass that in as a keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(64, 64, 64),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative would be to use a standard iterator for the test set, given we do not need to shuffle the data since we'll be outputting the predictions at the end of training.  \n",
    "The code would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "    (train, valid),\n",
    "    batch_sizes=(64, 64),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")\n",
    "\n",
    "test_iter = Iterator(\n",
    "    test, batch_size=64, device=device, sort=False, sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a `BucketIterator` looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.iterator.BucketIterator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.text]:[torch.LongTensor of size 31x64]\n",
       "\t[.numerical_labels]:[torch.LongTensor of size 64]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(train_iter))\n",
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = list(train_iter)\n",
    "valid_dataloader = list(valid_iter)\n",
    "test_dataloader = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([23, 64]), torch.Size([64]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = train_dataloader[0]\n",
    "it.text.shape, it.numerical_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dataset': <torchtext.data.dataset.TabularDataset at 0x1401d11d0>,\n",
       " 'fields': dict_keys(['text', 'labels', 'numerical_labels']),\n",
       " 'input_fields': ['text', 'numerical_labels'],\n",
       " 'target_fields': [],\n",
       " 'text': tensor([[  80,    3,   15,  ...,   22,   13,    3],\n",
       "         [3645, 2184,   38,  ..., 1083,    0,  120],\n",
       "         [   0,   10,   23,  ...,  839,    0,  976],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]),\n",
       " 'numerical_labels': tensor([0, 0, 0, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 2, 0, 0, 1, 2, 1, 0, 2, 0, 2, 0,\n",
       "         0, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 2, 0,\n",
       "         0, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(list(test_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "<class 'list'>\n",
      "[\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 31x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 30x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 38x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 5x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 8x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 34x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 8x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 8x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 53x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 35x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 4x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 31x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 32x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 3x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 39x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 4x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 33x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 41x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 45x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 37x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 32x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 33x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 30x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 36x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 44x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 33x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.LongTensor of size 53x32]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 32], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 4x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 31x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 38x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 30x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 35x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64]]\n"
     ]
    }
   ],
   "source": [
    "iterator = train_dataloader\n",
    "print(len(list(iterator)))\n",
    "print(type(list(iterator)))\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "<class 'torchtext.data.batch.Batch'>\n",
      "torch.Size([20, 64]) torch.Size([64])\n",
      "tensor([[   12,    21,   235,  ...,     3,    12, 10634],\n",
      "        [   11,  2835,    22,  ...,    20,  2899,    14],\n",
      "        [    5,  5580,   776,  ...,   325,  5804,    62],\n",
      "        ...,\n",
      "        [  292,   804,  7276,  ...,   587,    14,    40],\n",
      "        [   75,   387,   545,  ...,  1318,    17,    24],\n",
      "        [    2,     2,     2,  ...,     2,     2,     2]]) tensor([2, 1, 0, 2, 0, 2, 2, 1, 2, 1, 1, 2, 0, 0, 1, 0, 0, 2, 2, 1, 0, 0, 0, 1,\n",
      "        2, 0, 2, 2, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 2, 2, 1, 0, 2, 2, 0, 0, 2, 2,\n",
      "        2, 0, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 0, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = list(train_iter)\n",
    "next_ = train_dataloader[0]\n",
    "print(len(next_))\n",
    "print(type(next_))\n",
    "print(next_.text.shape, next_.numerical_labels.shape)\n",
    "print(next_.text, next_.numerical_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is, finally, our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMultiClass(nn.Module):\n",
    "    def __init__(self, vocab_length, hidden_dim, emb_dim=300, num_lstm_layers=1, num_linear_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_length, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_lstm_layers)\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_linear_layers)]\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.predictor = nn.Linear(hidden_dim, 3)  # 3 possible outcomes: negative, neutral, positive\n",
    "\n",
    "    def forward(self, seq):\n",
    "        emb = self.embedding(seq)\n",
    "        all_hidden, (h_n, c_n) = self.lstm(emb)\n",
    "        feature = h_n.squeeze() # taking the last output from the LSTM (we're dealing with many-to-one problem)\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "            feature = self.tanh(feature)\n",
    "         \n",
    "        preds = self.predictor(feature)\n",
    "        return preds\n",
    "\n",
    "\n",
    "model = LSTMultiClass(vocab_length=len(TEXT.vocab), hidden_dim=500, emb_dim=100, num_linear_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:43<00:00,  3.11it/s]\n",
      "INFO:__main__:Epoch: 1, Training Loss: 0.4095, Validation Loss: 0.4699\n",
      "100%|██████████| 134/134 [00:42<00:00,  3.12it/s]\n",
      "INFO:__main__:Epoch: 2, Training Loss: 0.3681, Validation Loss: 0.4746\n",
      " 20%|██        | 27/134 [00:07<00:29,  3.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c99bba3a62a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.unsqueeze(1).float()  # dependent variable (the supervision data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# [:,0].long())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-24ed27d10107>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mall_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# taking the last output from the LSTM (we're dealing with many-to-one problem)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "epochs = 3\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    model.train() # turn on training mode\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        x = batch.text  # independent variable (the input to the model)\n",
    "        y = batch.numerical_labels.long()  # dependent variable (the supervision data)\n",
    "        opt.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = loss_func(predictions, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.data * x.size(0)\n",
    "    epoch_loss = running_loss / len(train)\n",
    "\n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            x = batch.text\n",
    "            y = batch.numerical_labels.long()\n",
    "            predictions = model(x)\n",
    "            loss = loss_func(predictions, y)\n",
    "            val_loss += loss.data * x.size(0)\n",
    "\n",
    "    val_loss /= len(valid)\n",
    "    logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: binary classification, multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>\"\\n\\nHello Marcruhwedell, and Welcome to Wikip...</td>\n",
       "      <td>0d39d2eaa40a6241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>...that's why I did ....cheers,   (talk · cont...</td>\n",
       "      <td>0d3b57f3b2db3f03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>No, it's not a delayed reaction\\n\\nI just happ...</td>\n",
       "      <td>0d3bb1f12d90a6a2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>\"\\n\\nA slight difference with you\\nI have to d...</td>\n",
       "      <td>0d3be13abf1bec52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>\"\\n\\nNonsense.  The truth is NO schools give s...</td>\n",
       "      <td>0d3c720db297b5a3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text                id  \\\n",
       "0     Explanation\\nWhy the edits made under my usern...  0000997932d777bf   \n",
       "1     D'aww! He matches this background colour I'm s...  000103f0d9cfb60f   \n",
       "2     Hey man, I'm really not trying to edit war. It...  000113f07ec002fd   \n",
       "3     \"\\nMore\\nI can't make any real suggestions on ...  0001b41b1c6bb37e   \n",
       "4     You, sir, are my hero. Any chance you remember...  0001d958c54c6e35   \n",
       "...                                                 ...               ...   \n",
       "4995  \"\\n\\nHello Marcruhwedell, and Welcome to Wikip...  0d39d2eaa40a6241   \n",
       "4996  ...that's why I did ....cheers,   (talk · cont...  0d3b57f3b2db3f03   \n",
       "4997  No, it's not a delayed reaction\\n\\nI just happ...  0d3bb1f12d90a6a2   \n",
       "4998  \"\\n\\nA slight difference with you\\nI have to d...  0d3be13abf1bec52   \n",
       "4999  \"\\n\\nNonsense.  The truth is NO schools give s...  0d3c720db297b5a3   \n",
       "\n",
       "      toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0         0             0        0       0       0              0  \n",
       "1         0             0        0       0       0              0  \n",
       "2         0             0        0       0       0              0  \n",
       "3         0             0        0       0       0              0  \n",
       "4         0             0        0       0       0              0  \n",
       "...     ...           ...      ...     ...     ...            ...  \n",
       "4995      0             0        0       0       0              0  \n",
       "4996      0             0        0       0       0              0  \n",
       "4997      1             0        0       0       1              0  \n",
       "4998      0             0        0       0       0              0  \n",
       "4999      0             0        0       0       0              0  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = os.path.join(os.getenv('DATA_DIR'), 'toxic_comments_dataset')\n",
    "train_dataset = pd.read_csv(os.path.join(folder, 'train_dataset.csv'))\n",
    "valid_dataset = pd.read_csv(os.path.join(folder, 'valid_dataset.csv'))\n",
    "test_dataset = pd.read_csv(os.path.join(folder, 'test_dataset.csv'))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize='spacy', lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comment_text', <torchtext.data.field.Field at 0x155856940>),\n",
       " ('id', None),\n",
       " ('toxic', <torchtext.data.field.Field at 0x140c964a8>),\n",
       " ('severe_toxic', <torchtext.data.field.Field at 0x140c964a8>),\n",
       " ('obscene', <torchtext.data.field.Field at 0x140c964a8>),\n",
       " ('threat', <torchtext.data.field.Field at 0x140c964a8>),\n",
       " ('insult', <torchtext.data.field.Field at 0x140c964a8>),\n",
       " ('identity_hate', <torchtext.data.field.Field at 0x140c964a8>)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [\n",
    "    (\"comment_text\", TEXT),\n",
    "    (\"id\", None),  \n",
    "    (\"toxic\", LABEL), \n",
    "    (\"severe_toxic\", LABEL),\n",
    "    (\"obscene\", LABEL),\n",
    "    (\"threat\", LABEL),\n",
    "    (\"insult\", LABEL),\n",
    "    (\"identity_hate\", LABEL)\n",
    "]\n",
    "\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TabularDataset.splits(\n",
    "    path = folder,\n",
    "    train = 'train_dataset.csv',\n",
    "    validation = 'valid_dataset.csv',\n",
    "    test = 'test_dataset.csv',\n",
    "    format = 'csv',\n",
    "    skip_header = True,\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(64, 64, 64),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.comment_text),\n",
    "    sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dataset': <torchtext.data.dataset.TabularDataset at 0x13faa97f0>,\n",
       " 'fields': dict_keys(['comment_text', 'id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']),\n",
       " 'input_fields': ['comment_text',\n",
       "  'toxic',\n",
       "  'severe_toxic',\n",
       "  'obscene',\n",
       "  'threat',\n",
       "  'insult',\n",
       "  'identity_hate'],\n",
       " 'target_fields': [],\n",
       " 'comment_text': tensor([[ 2051,  9599,     5,  ...,  4508,  1514,  7263],\n",
       "         [10474,     0,    25,  ...,     4,   506, 11587],\n",
       "         [    0,    25,   118,  ...,     7,    25,    95],\n",
       "         ...,\n",
       "         [   19,    29,    16,  ...,  3461,  1630,     0],\n",
       "         [ 2861,     0,    56,  ...,     2,    23,  6299],\n",
       "         [    2,     2,     5,  ...,     1,     1,     1]]),\n",
       " 'toxic': tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'severe_toxic': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'obscene': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'threat': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'insult': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'identity_hate': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(list(train_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = list(train_iter)\n",
    "valid_dataloader = list(valid_iter)\n",
    "test_dataloader = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTMBinary(nn.Module):\n",
    "    def __init__(self, vocab_length, hidden_dim, emb_dim=300, num_lstm_layers=1, num_linear_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_linear_layers = num_linear_layers\n",
    "        self.embedding = nn.Embedding(vocab_length, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers = num_lstm_layers)\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_linear_layers)]\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_dim, 6)  # 6 different binary labels\n",
    "        self.tanh = nn.Tanh()\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        emb = self.embedding(seq)\n",
    "        all_hidden, (h_n, c_n) = self.lstm(emb)\n",
    "        preds = h_n.squeeze()\n",
    "        for layer in self.linear_layers:\n",
    "            preds = self.tanh(layer(preds))\n",
    "        preds = self.output(preds)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "model = LSTMBinary(vocab_length=len(TEXT.vocab), hidden_dim=500, emb_dim=100, num_linear_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 33/79 [03:25<04:45,  6.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-3c75588e44e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 3\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        x = batch.comment_text\n",
    "        y = torch.cat([getattr(batch, label).unsqueeze(1) for label in labels], dim=1).float()\n",
    "        opt.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = loss_func(predictions, y)\n",
    "        loss.backward()\n",
    "        opt.step()     \n",
    "        running_loss += loss.data * x.size(0)\n",
    "    epoch_loss = running_loss / len(train)\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            x = batch.comment_text\n",
    "            y = torch.cat([getattr(batch, label).unsqueeze(1) for label in labels], dim=1).float()\n",
    "            predictions = model(x)\n",
    "            loss = loss_func(predictions, y)\n",
    "            valid_loss += loss.data * x.size(0)\n",
    "\n",
    "        valid_loss /= len(valid)\n",
    "    logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 105x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 60x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 239x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 64x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 35x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 88x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 45x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 27x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 75x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 270x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 17x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 39x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 84x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 19x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 8]\n",
       " \t[.comment_text]:[torch.LongTensor of size 2089x8]\n",
       " \t[.toxic]:[torch.LongTensor of size 8]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 8]\n",
       " \t[.obscene]:[torch.LongTensor of size 8]\n",
       " \t[.threat]:[torch.LongTensor of size 8]\n",
       " \t[.insult]:[torch.LongTensor of size 8]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 8], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 9x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 129x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 9x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 399x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 50x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 49x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 315x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 165x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 96x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 8x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 154x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 20x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 146x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 5x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 13x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 16x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 21x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 26x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 28x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 43x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 42x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 25x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 6x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 69x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 36x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 66x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 11x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 12x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 14x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 31x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 195x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 10x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 23x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 78x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 212x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 81x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 139x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 47x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 16x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 100x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 33x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 18x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 32x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 54x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 29x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 92x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 986x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 110x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 58x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 62x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 55x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 7x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 116x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 52x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 178x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 24x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 624x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 14x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 37x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 34x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 123x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 72x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 40x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64], \n",
       " [torchtext.data.batch.Batch of size 64]\n",
       " \t[.comment_text]:[torch.LongTensor of size 22x64]\n",
       " \t[.toxic]:[torch.LongTensor of size 64]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       " \t[.obscene]:[torch.LongTensor of size 64]\n",
       " \t[.threat]:[torch.LongTensor of size 64]\n",
       " \t[.insult]:[torch.LongTensor of size 64]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 64]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = list(train_iter)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([239, 64])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[2].comment_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "april 2006 ( utc ) \n",
      "\n",
      "  wikipedia : colours \n",
      "\n",
      " much thanks for fixing / improving the table code .  \n",
      " no reason , maybe add the info where you found the values , is that in <unk> ?   \n",
      " all the styles are <unk> . highly - repetitively . if you know your way around mediawiki , i 've been asking everywhere for a css guru to clean up all the <unk> on the main page wikicode . ( like we do nt all have enough to do already , eh ? ;) so many projects , so few <unk> ... <unk> , 11\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "screwjob \n",
      "\n",
      "    hey i noticed your comments on the montreal screwjob discussion page . i decided since nobody except someone with no account objected to what you said . i would atleast change the page a little to make it fair . i just wanted to tell you because i thought you would like to know . i only changed a few words at the top of the first paragraph and added something to the second one . if your not bothered that s fine but i thought since you were fighting for and nothing was even said , i might aswell tell you .\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "no , taekwondo is in the first sem this year ( or maybe in the succeeding years ) due to the placement of volleyball in the second sem . i do n't know the true results of the taekwondo tournament yet . \n",
      "\n",
      " swimming details \n",
      " please be informed that rankings in the swimming championships is based on total number of points <unk> by the team , not solely on medal <unk> . and if possible , please include all medals . it is suggested that the table be put with these headings : team | points | gold | silver | <unk> | total\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      " i read that quality article , reference number 3 where he states , \" \" it 's not because i died in the 9/11 attacks that put my career on hold , it 's because my financial <unk> pulled out \" \" he 's obviously a genius . also the team owner or whatever he was stated the \" \" the newton - john name goes a long way in motor <unk> as everyone knows who she is \" \" read your own references before getting on your high horse . go grab yourself a tube of <unk> and fuck <unk>   \"\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "the multitude of sources simply can not be ignored . there are several articles stating the environmental impact . i will actually expand the section in the article because the <unk> is so big , people should know as a part of an encyclopedia the river bed violations . i would disagree with the arguments given above and that the edit should have more expansion within the article as a half dozen citations stating there are environmental issues . also there is a persistent   conflict of interest from the users above but from their follow - ups , there intentions will be clear .\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "terrorist \n",
      "\n",
      " you seem like a terrorist sir . are you penises as small as they say they are ? i have corroborated it with a cite something you never bothered to do . you may keep your comments about <unk> as long as you cite it even though it has no place on this page . in turn please do n't remove what i have written about the   <unk> nature of conflict . \n",
      "\n",
      " and why do all muslims ruin <unk> good name with your allah o <unk> crap . he was a good king and you should not sully his image .\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "image copyright problem with image:23rd february 2008 3.jpg \n",
      "\n",
      " thank you for uploading image:23rd february 2008 3.jpg . however , it currently is missing information on its copyright status . wikipedia takes copyright very seriously . it may be deleted soon , unless we can determine the license and the source of the image . if you know this information , then you can add a copyright tag to the image description page . \n",
      "\n",
      " if you have any questions , please feel free to ask them at the media copyright questions page . thanks again for your cooperation . ( ) ( talk )\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "contested deletion \n",
      "\n",
      " this page should not be speedy deleted because ...   \n",
      "\n",
      " .. because it 's nominator is a follower of a   that is on a crusade to smear this person . speedily deleting this makes zero sense , as it is a real product released 12 months ago to compete on the cloud front against amazon <unk> . surely , you understand this . being in beta does n't make it any less of a usable product ; i merely requested that the <unk> team share more <unk> . if you get an invite , you 'll see what i mean .\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "it is common knowledge that <unk> ( but not karaite jews ) <unk> descent from khazars , but what they do not realize is that we believe the <unk> - <unk> ( on - <unk> ) <unk> came ( mixing with <unk> mercenaries from <unk> and <unk> ) our <unk> ancestors were in fact the <unk> of the lost ten - tribes of israel who formed our <unk> . it is precisely for this reason that karaite jewish scholars like jacob ben <unk> of <unk> and <unk> ben ali or <unk> ben <unk> call us bastards in their writings . \n",
      "\n",
      " the following editor (\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "hello dan ! \n",
      "\n",
      " i am esther and i from spain , i have <unk> hear and i do nt like the <unk> , but i like <unk> . i love your films and i think you are excellent ! , i do nt like boys to his hear is fair , but i like the boys to his hear is black . i <unk> but i do nt talk very good spanish , sorry , <unk> . \n",
      " please , contest me ! \n",
      " thank you ! \n",
      " esther <unk> , \n",
      " <unk> : <unk> got 13 years old . \n",
      " good bay !\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      " thanks . i can see that violating clearly stated wikipedia policy is no problem with some people , as is now being reviewed elsewhere . finishing up one other wiki project <unk> and then spending the rest of the day on an important personal blog entry .   then i 'll go find other discussions of james <unk> ' fascinating discussion of the use of the term \" \" jewish lobby \" \" so that i do n't have to put back up the deleted dissident voice article right away per talk : <unk> : <unk> . \n",
      " carol moore   talk \"\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "  the above is a <unk> of the citation provided .   for avoidance of doubt , here are the full citations : * <unk> : \" \" the pentagon labeled that incident \" \" workplace violence \" \" * <unk> \" \" the army determined that there was sufficient evidence to conclude hasan ( was ) “ inspired or motivated by the foreign terrorist <unk> , we have the department of defense of the united states asserting that this \" \" workplace violence \" \" that is \" \" inspired or motivated by the foreign terrorist organisation \" \" .    \" <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "major premise : taking steroids is cheating ( <unk> rules ) minor premise : barry bonds admits to taking steroids .   conclusion : barry bonds is a cheater . \n",
      " major premise : saying something that is n't true is lying . minor premise : barry bonds <unk> said that he had not taken steroids .   conclusion : barry bonds is a liar . \n",
      " major premise : people are indicted by federal grand <unk> for committing crimes .   minor premise : barry bonds was indicted by a federal grand <unk> .   conclusion : barry bonds is a criminal . <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "that last line ( the new version is better ) is <unk> .   this ' <unk> ) version just <unk> the issue and makes the article less simple to read .   tell me , why must boston be mentioned as a cultural and business hub at all ?   this seems to be at the heart of your bias .   it appears as you want the viewers to get from reading it that boston is somehow the nerve center and city that new england <unk> around and looks up to .   why must this illusion be on the page ? <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "this is absolutely false .   i can not believe what 's going on .   first i make an edit that obviously conforms to wikipedia standards and is a helpful contribution , and it gets edit - warred down by 2 or 3 people who do n't care about what belongs on wikipedia .   and now you make ridiculous false accusations of sock - puppetry because someone else agrees with my edit which was obviously right in the first place ? !   i certainly need an admin to reconsider what 's going on - maybe i need to appeal this . <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "i agree with <unk> , though i began attempting compromise last night considering no one else was backing me up on this .   in the interest of building consensus , however , i will note that political <unk> , in the absence of reliable third party sources , are inappropriate in this article .   i will also note , again , that some <unk> here appear to be confusing the electoral - vote website with the electoral - vote author .   this wikipedia article is about the website , not the author , who has an article of his own . <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "npov tag \n",
      " an anonymous user removed the npov tag yesterday without any comment . i do n't think the article is quite ready for that yet , because its problems are not really resolved . it retains a poison pen <unk> from what seemed to be edits from anonymous users with an agenda , and we ended up with some odd compromises like a <unk> <unk> mention in the first paragraph ( she 's really not important enough imho ) , <unk> sources like <unk> , continued <unk> of race , etc . others may not agree , but let 's see . <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      " but what neither <unk> nor yourself have been willing to tell us is : \" \" what is this real speed of light ? \" \" can we measure it ? the measurement of the speed of light ( as we all seem to have agreed on its definition ) presents no problem at all , so long as you can provide a length standard that is sufficiently precise . the speed of light is still <unk> , at inner solar system <unk> at least , and to <unk> <unk> . all of this after 1983 .   ( talk ) \" <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  question \n",
      "\n",
      " i 'm sorry , i should have called you \" \" mr . phelps \" \" from the start . to call you by your christian first name was certainly uncivil . now please , mr. phelps , tell us why you did it . tell us why you did wtc . you <unk> a testimony , did n't you ? you either received a testimony or you just hate our freedom . perhaps the testimony told you to hate our freedom . is that it , mr. phelps ? is that why you did wtc ? <unk>   \" <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      " the wp : airlines is only a suggestion , it is not policy . guess what outside of your flight <unk> community the average user of wiki does not care about destinations and my edit improves the article and still allows people interested in destinations to click the \" \" show \" \" . wp : airlines is not policy , it is a suggestion . maybe you should care more about the introduction and history section of articles instead of the <unk> and <unk> size ( both of which are to minimal interest to the average public ) <unk>   \" <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "actually , undue weight can also results in zero weight .   as it is the whole issue is insinuation and <unk> of given events .   just because the article does n't outright say that edwards is the father because there was no father listed , it is quite clear that this is the intent .   now , editors familiar with me consider me to be a <unk> fanboy and <unk> regarding conservatives and republicans , so my objection to this is clearly not a partisan point of view .   that said this is in violation of several wp policies . <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "<unk> long time , no talk \n",
      "\n",
      " i was just <unk> around on wiki , and suddenly i thought of you . so anyway , how ya going ? i hope we can get along now . \n",
      "\n",
      " anyway , there is an issue that i have , that has to do with wikipedia . a user on here had been stalking me and asking some <unk> questions . do you have any ideas of what i could do . i know that user is also from australia . could the police here do any thing ? \n",
      "\n",
      " cheers for now ! \n",
      "\n",
      " <unk> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  <unk> as best or only ? \n",
      "\n",
      " you seem to think that she is the only female pro , but you are mistaken . there are many others such as : berry , <unk> , navy , january , <unk> , <unk> , <unk> , <unk> , anna , <unk> , fish , <unk> , precious , puzzle , vitamin , <unk> , <unk> , <unk> and others . also , if she is the only pro , why do u think they still <unk> hold all female sc events in korea ? so yes , i will change it back \" <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  cfr is a perfectly accurate source . \n",
      "\n",
      " the \" \" council on foreign relations \" \" ( <unk> ) is a recognized , legitimate think - tank that studies international <unk> for the united <unk> govt . to claim they are \" \" not credible \" \" is idiotic . \n",
      " they are as legitimate as any online newspaper or other source . they have studied northern ireland , and <unk> a list of <unk> / <unk> killed by the ira / <unk> , and vice versa . these facts are good and will be included in the article . \" <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  \" \" life \" \" vs. \" \" good behaviour \" \" \n",
      "\n",
      " this article needs a lot more citing , in general . however this recent edit seems problematic . it needs to be cited , or better , <unk> to say something like \" \" good behaviour \" \" ( us constitution : article 3 , section 1 , second sentence ) which is effectively \" \" life \" \" . there are many sources for the constitution , here is one i found : emory law library hope that helps .   + + : t / c \" <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      " the wikipedia organization has been established and operated as a non - profit corporation under the federal laws of the united states of america , and furthermore , it is operated by internet servers located in the united states , and furthermore , it operates under the copyright laws of the united states . so , whose spellings should be used here ? ( and that is a <unk> question , too ) . it someone wants to use british spellings , etc . , let them establish their own \" \" <unk> \" \" and use that <unk>   \" <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "tbma \n",
      "\n",
      " i think its very unfair to compare my edits with those of tbma / ymb29 , first i have been the main contributor to this article , i have <unk> the <unk> <unk> in discusion all in line to improve this article , this is not the case with tbma / ymb29 who just last week consider the whole article a hoax , also today i really <unk> myself not doing any of the <unk> acts of misconduct , so i think your <unk> of me is <unk> unfair . <unk> ( talk ) <unk> , 24 july 2010 ( utc <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "tesla was a serb , born in croatia . the article is not objective , and i think objectivity is important . one harmless example of that is presenting banknote of serbia with his image and not presenting the croatian one . apparently this wiki page is edited by serbian wiki page , and that 's why it 's not objective . i suggest that the page should be edited by another wiki project . that 's how we will stop counting tesla 's blood cells and manipulating with the history . give tesla in the hands of neutral party . thanks . <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  <unk> are not always <unk> ! \n",
      "\n",
      " under geometry it is stated that \" \" a <unk> always has six <unk> arms . \" \" this assertion is simply not true ! according to kenneth <unk> , \" \" the rather unattractive <unk> <unk> are by far the most common variety . \" \" <unk> someone really need to take a look at his site and get facts off of it because i still see a decent number of <unk> on this page . ( forgive me i m new at this and do nt want to edit anything ) \" <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      " yes yes and yes ! i do n't know the songs directly by name , but if it were to play , i know i 'd dance my ass off cause songs like those are <unk> ! ! ! d i was at a <unk> bar this weekend , and \" \" <unk> \" \" came on , and me and my friend got up and danced all over the place ! <unk> ! ! ! i might be a white - boy , but i sure as hell do n't dance like one ! <unk>   stack stack stack \" <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "also , i see that you have experience with what it takes for film articles to be ga , so you may have already known about wp : plot and a bit of what i mentioned above in this section . your editing to the avatar ( 2009 film ) article has been a little ( just a little ) to the contrary on these matters , though . \n",
      "\n",
      " i do not mean to sound as though i am condescending to you or anything of that nature . i know that you do great work here , and i appreciate that . <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "  \" \" diverse views \" \" means editors with a variety of perspectives and general outlooks . \" \" experienced \" \" means the person in question has been around for a while , is very familiar with policy , and has some experience with related topic areas . with a bit more focus and structure with the involvement of a few experienced hands will make it much easier for uninvolved administrators to decipher what 's going on and identify disruptive editors . if i can further clarify or answer any other questions , please let me know .    \" <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "well , actually i 'm more into statue and <unk> <unk> ( <unk> ) , but it 's all just three sides of the same coin really . how do i know so much about it ? i 've always been into statues and <unk> , and when i first got connected to the internet i was delighted to learn that there was a whole online community who shared my interest . if you care to research the subject further , here 's a page linking to lots of related sites : \n",
      "\n",
      " <unk> \n",
      "\n",
      "  <unk> , 17 nov 2004 ( utc ) <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "gibraltar history \n",
      "\n",
      " gibraltar has a long history and the failed spanish attempt to ' <unk> ' the territory forms a small part of its modern history .   anyone who doubts that the gibraltarians reject the claim or any proposal of joint sovereignty should read the section on the 2002 referendum . \n",
      "\n",
      " as regards claims that terrorists were ' murdered ' here , the gibraltar constitution defines what murder is , and the inquiry held in gibraltar determined that it was <unk> killing . visitors intending travelling with bombs should note that part of the constitution is still in place . <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      " the quote is exclusive to the <unk> piece though and , as i say goes to the \" \" did they jump or were they pushed ? \" \" issue - the quote suggests they jumped . the variety piece ( a blog ? ) says they were fired but does n't quote a source and that could just their spin . again we should be careful about our wording as there seems to be different takes on this . for a fully balanced approach we 'd need something directly from the production side of things . (   ) \" <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "lack of balance \n",
      "\n",
      " this article is seriously out of balance .   it would benefit greatly from the introduction of additional information about the negative aspects of the t-34 .   just for the record , this sort of balance problem is not <unk> when there is an over <unk> on the works of a particular author .   for example , testing and evaluation of a t-34 by the us army <unk> dept . exposed some very serious problems with the tank .   this type of information should be included in the article to bring it back into balance . <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "for your information sir , feminism is merely the belief that women are equal to men . you 've just stated that he does not care about the sex or race of individuals . he obviously disagrees with many feminists on certain issues , particularly abortion , but he believes men and women are equal , and thus , is a feminist . this article <unk> <unk> and makes it sound like he believes unattractive women should not have access to mainstream society . if you do not think he is a sexist , than why should this portion not be corrected ? <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "<unk> = notice|text = please note : i am not a bot . i use <unk> , which allows me to make a lot of edits in a short amount time . i will answer your message after i am done . thank you . } } \n",
      " { { <unk> = notice|text = this user talk page is watched by friendly talk page <unk> which means that someone other than me might reply to your message . their input is welcome , and their help with messages when i ca n't reply in a clear or quick manner is appreciated . <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "raffaele 's request for a lawyer while being <unk> on nov. 5 was refused , as was his request that he be allowed to call his father . page <unk> \n",
      "\n",
      " amanda was denied food , water , bathroom breaks and <unk> during the all night <unk> on nov. 5 - 6 .. page <unk> \n",
      "\n",
      " <unk> : actually , now looking at murder in italy , i see that there is a ton of additional information about both amanda and raffaele 's <unk> that could be included . so i will work on listing all that in more detail tomorrow . <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      " then the young man said to the platypus , \" \" son do n't drink your friends or their shoes . \" \" the platypus looked up at the man without understanding and bit off his toe . the young man said , \" \" well i never did tell you not to eat my toes . do n't eat my toes . \" \" the platypus went to the corner of the box and ate some <unk> and the young man became quite aquainted with the platypus , but the platypus was not <unk> with him . \n",
      " \" <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "its also <unk> against guidelines to sneak in <unk> links in the discussion section , as froman has done with his youtube propaganda link above , but as evidenced throughout this page , the rules do n't seem to be applied to froman . why do n't you just go whole hog and add it to <unk> 's main page ? there are plenty of unencyclopedic sources there now and no admin seems to have a problem with any link , anonymous , left wing , blogs , whatever you care to use seems to be fair game for froman . <unk> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" = = = typical <unk> \n",
      "\n",
      " most of the bio , which was not written by me , is about the various martial arts i have encountered , never said i was a master of all of them . one paragraph at the top mentions three of my thirty books , i figure that 's pretty humble , lol only two references to getting some <unk> overseas , nothing here . but you read that as \" \" <unk> ? \" \" take some reading comprehension courses . obviously , you are a fool , lol \n",
      "\n",
      " <unk> kim \n",
      "\n",
      " \" <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      " i do not want to keep this ongoing , but i will respond to a few comments you made .   i never said i was a newbie .   i said i was \" \" inexperienced \" \" , as i log on <unk> .   i thought i was in sandbox , and <unk> made the change .   again , for someone who made two very small errors , you should assume good faith .   if you feel it is vandalism , ask the user what is going on before jumping to judgement . \n",
      "   \" <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "goodday : cu seems to be <unk> only in extremely serious cases . i came to an agreement with administrator <unk> ? ? to register with all previous ( unwarranted ) complaints about ip hopping then to be taken off the table . once i registered i have only used <unk> . i can give no other <unk> and do n't see why i need to . o fenian et al use any and every bureaucratic device to wear down their ' opponents ' . however you can see an example in the peter <unk> article where they were seen off . <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "i am trying to explain to you once and for all that neil armstrong landing on the moon is just as fictitious santa <unk> appearing on <unk> or <unk> and his <unk> <unk> . how dare you <unk> cyberspace with outrageous lies that suggest it was otherwise and that deadly diseased celebrity liars like armstrong 's and aldrin 's and <unk> 's were telling the truth about it all . what if young and <unk> children looked up apollo on wikipedia and all they see is nothing but your deadly mistaken <unk> of late 60 's hoaxes presented as scientific achievements ? <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "um , sure ecw <unk> mean something , but they do n't mean hardcore wrestling , and least not entirely .   does the name chris <unk> , <unk> <unk> jr , dean <unk> , <unk> <unk> , chris <unk> .... i could go on and on and on mean anything to ya ?   ecw was more then hardcore wrestling , it was the <unk> of everything we see on tv today .   not taking away anything from <unk> , because it was great .   but it was almost exclusively hardcore garbage wrestling .   ecw was n't . <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      " ok , here it is , peer viewed for 12 months and published at academic archive on - line ( <unk> ) , \n",
      " on a permanent <unk> : <unk> : <unk> : se : <unk> : <unk> \n",
      "\n",
      "  wallin h. 2008 , <unk> p : an investigation of friction graphs ranking ability regarding the <unk> phenomenon in dry <unk> contact   ( <unk> material transfer and friction ) , a free pdf document available here or <unk> found here or at <unk> here use search <unk> & <unk> wallin \" \" or the direct <unk> link here \n",
      "\n",
      "  \n",
      "\n",
      " \" <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "transistor  \n",
      "\n",
      " the transistor page does not have a principle of operation section , because \n",
      "\n",
      " <unk> physicists know about it but do n't want to share it \n",
      " <unk> just do n't know how it works \n",
      "\n",
      " i was thinking about it and came up with a fairly decent concept of how it works , i put it on wiki , i think the guys got jealous , and here i am , discussing pretty much nothing . \n",
      "\n",
      " ps : and my language is pathetic , you naughty naughty <unk> .. i'll eat you ... : ) ) ) <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "( utc ) this sentence is an editorial opinion and the information about financing is already included . being an editorial opinion does not <unk> it per se , but taking account the fact that this is an article about the worldwide company with more than <unk> history , this opinion about one specific aspect related to the company , voiced by non - mainstream magazine , will have undue weight . it may be relevant in some other article ( e.g. focused to the climate change denial financing ) , depending the context .    <unk> , 8 january 2016 <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "all the references state the plane hit turbulence , as wikipedia is built <unk> references , that is what i put , i have no <unk> with continental , and it seems you do ! turbulence is the reason as witnesses on the plane describe aswell as the pilot ! so make any accusations you want but for now , turbulence is the main reason and factor of the flight . now just remember , follow the rules and stop saying major news broadcasters like cnn and bbc are false because you ' seem to think you are right ' ! <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "curtisnaito 's article edits are a disruptive , and in the last few months he has been campaigning for me to get banned from wikipedia for criticizing these edits . he has managed to sneak several poorly - sourced articles past the ga process , and these articles should be <unk> in accordance with the ga criteria . all the other users who have commented on the issue will agree with me . my offering to not request these <unk> if curtisnaito stops following me and harassing me was a peace offering , not a threat .   ( 聖やや ) <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      " i disagree . slang should only be used in direct quotes . and even though this could be a direct quote , the rest of the plot summary is n't ( and should n't be ) . so the word \" \" kiss \" \" is better . incidentally , if this is about <unk> , it is n't important enough to be in the summary to begin with , but as per the section above this one , it 'll be futile trying to remove it now .   ( talk • contribs •   count ) \" <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "circumcision advocacy \n",
      "\n",
      " jake , you 've twice reverted on circumcision advocacy without offering the slightest bit of explanation .   some people might consider this to be intentional edit - warring , but i 'm going to assume good faith and instead politely remind you that edit comments are very important and that reverting without a stated reason is not considered to be a good practice .   for now , i 'm going to undo your changes again , but if you have a good reason , all you have to do is explain it .   thanks ! <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "tfa - dec 23rd \n",
      "\n",
      " sorry to bother you on your talk page . a few users have requested that the newly <unk> feature article early life of joseph smith , jr. be used on december 23rd , the <unk> anniversary of his birth - and worked hard to get it to feature status before that anniversary . since the 23rd is less than one week away , i was wondering if you could make a decision whether you can <unk> it out for the current article <unk> to appear that day , <unk> of australia . thx in <unk> . <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "the truth of the matter is , you have failed to provide evidence there is no chang at the <unk> project . again , refer to the sources i 've already referenced that attempt to validate chang 's existence , but <unk> that also do n't say it 's possible he does exist in other parts of the project . research it deeper ; do n't just dismiss something at quick glance or limited <unk> it until you ca n't go any <unk> then research why ; it may lead you to even deeper areas than you first saw ... - <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "there was a comment on this page that cornell did n't win the tony for <unk> and <unk> and points to the american theatre wing web site for confirmation . the person who posted that is incorrect : cornell did win the tony that year along with <unk> <unk> and judith anderson ( there were multiple winners that year ) . the web site makes it appear that anderson is the <unk> nd the other two are nominees only because she is listed first on the list with the other two under her , but they all won the award . <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  thank you - my point exactly is what you said \" \" users are expected to <unk> with others \" \" . regarding my reverts : as can be seen in the history of that article , i made two reverts as opposed to four by that other user . also , he immediately accused me of vandalism . and repeatedly . his behaviour on my talk page is something that i can easily perceive as harassment - he even claims i \" \" indirectly threaten \" \" him and others who follow the same ideology .    \" <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "homosexuals are intent on <unk> their behavior . therefore , they will seize any opportunity to suggest that famous persons are gay . in this way , their way of life will be received as being not only harmless , but perfectly normal . as generations of people grow from childhood to <unk> and are exposed to this strategy , they increasingly develop a <unk> for homosexuality . in this way , it is now almost universally accepted as a diverse or alternate behavior or way of being , similar to a religion , race ,   or nationality . <unk> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      " <unk> <unk> \n",
      " <unk> , my man , thanks for your quick actions and additions re . this article ( just a stub i created <unk> ago to get rid of red links on bowie / pop articles ) . what is it that <unk> people like us to defence and rock ( i think nick d. is a member of the military and music projects as well ) . then again , the <unk> for \" \" <unk> for life \" \" came from bowie 's interpretation of an armed forces radio theme ... cheers ,   \" <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      " very informal language ... \n",
      "\n",
      " i feel surprised by the very informal language used in this article ( \" \" .. the <unk> was pulled .. \" \" ) and feel that the interpretation of events guiding \n",
      " the decision to <unk> life support is hit and miss at best ( \" \" .. because noone wanted him to die during the championship .. \" \" ) . \n",
      " i think this should be <unk> by someone more in the know about what really happened and with a more objective <unk> on these tragic events . \n",
      "\n",
      " bert \" <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "what do you consider primary sources in this context ?   she notes the obvious <unk> passages in rydberg 's poetry and fiction , which are primary sources for literary criticism .   the details of rydberg 's sexually troubled childhood and his <unk> relationship with women were presumably taken from his standard biographical sources .   i have no idea what sources <unk> and <unk> used ; i ca n't ask them .   again , no other scholar has questioned the accuracy of their conclusions , which seems to me to be the definition of a scholarly consensus . <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "arabic wiki = = \n",
      "\n",
      "  hallo rouge admin , you know a little perhaps about <unk> ! no hope for the ar wiki for now , the majority will always write history ! , but it 's <unk> to have a rouge admin by one 's side . ar wiki criticism , is n't a blame on wiki in whole , consider other products <unk> . \n",
      " how do u regard this paragraph : one arabic journalist in netherlands , bla bla bla ? ? ? ? ? ( sure , it comes from a doctor ! )   \n",
      "\n",
      " = = <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  need administrative help \n",
      "\n",
      " i have been blocked , <unk> . read the above request . i tried to explain everything there . <unk> use of administrative privileges ought to not be overlooked . ask administrator   if i was harassing him to such an extent where i needed to be blocked from editing , that too for 48 <unk> . for goodness ' sake . what happened to do n't bite the newcomers ? please talk to bwilkins . see the talk page of   he has a <unk> of being unreasonable with <unk> . here \" <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "i 've revised the translation from the spanish by going back to this earlier edit .   subsequent edits on the spanish - language wikipedia seem to have been intended mainly to insert links , but have primarily <unk> in deleting chunks of the original text , rendering the result both <unk> and somewhat <unk> .   i 've taken 's translation and incorporated the previously omitted text , and gone over the whole to make it more <unk> in english .   it 's not quite there , yet , but is hopefully now <unk> .   ( ) <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(l[0].comment_text[1])):\n",
    "    print(\"\\n NEW SENTENCE \\n\")\n",
    "    print(' '.join([TEXT.vocab.itos[x] for x in l[0].comment_text[:,i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
