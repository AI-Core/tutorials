{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext tutorial\n",
    "\n",
    "\n",
    "Made by [Andrea Sottana](http://github.com/AndreaSottana) and adapted from the following [tutorial](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)\n",
    "\n",
    "This notebook shows how you can use your own custom dataset starting from the raw form (for example a .csv file) with the `torch` library, to easily do some cleaning and preprocessing (such as tokenization), build a vocabulary, and ultimately create an `Iterator` or `BucketIterator` object that you can feed it into a neural network built using the `torch` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset, Iterator, BucketIterator\n",
    "from torchtext import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=20)  # shows info level loggings (we'll use this to print the loss during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data `Field` specifies how you want a certain field to be processed, for example whether you want to tokenize it, lower case the text etc. In our case we have one text field, and one label field for the sentiment analysis, which we represent with an integer, where 0 is negative, 1 is neutral and 2 is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize='spacy', lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>numerical_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>No surprises .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels  \\\n",
       "0     The Rock is destined to be the 21st Century 's...  positive   \n",
       "1     The gorgeously elaborate continuation of `` Th...  positive   \n",
       "2     Singer\\/composer Bryan Adams contributes a sle...  positive   \n",
       "3     You 'd think by now America would have had eno...   neutral   \n",
       "4                  Yet the act is still charming here .  positive   \n",
       "...                                                 ...       ...   \n",
       "8539                                    A real snooze .  negative   \n",
       "8540                                     No surprises .  negative   \n",
       "8541  We 've seen the hippie-turned-yuppie plot befo...  positive   \n",
       "8542  Her fans walked out muttering words like `` ho...  negative   \n",
       "8543                                In this case zero .  negative   \n",
       "\n",
       "      numerical_labels  \n",
       "0                    2  \n",
       "1                    2  \n",
       "2                    2  \n",
       "3                    1  \n",
       "4                    2  \n",
       "...                ...  \n",
       "8539                 0  \n",
       "8540                 0  \n",
       "8541                 2  \n",
       "8542                 0  \n",
       "8543                 0  \n",
       "\n",
       "[8544 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember to source the .envrc file in the terminal before launching this notebook to \n",
    "# ensure it can use the environment variables correctly.\n",
    "\n",
    "folder = os.path.join(os.getenv('DATA_DIR'), 'movie_review_dataset')\n",
    "train_dataset = pd.read_csv(os.path.join(folder, 'train_dataset.csv'))\n",
    "valid_dataset = pd.read_csv(os.path.join(folder, 'valid_dataset.csv'))\n",
    "test_dataset = pd.read_csv(os.path.join(folder, 'test_dataset.csv'))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells the field what data to work on. Note that the fields we pass in must be in the same order as the columns in our csv file. For the columns we don't use, we pass in a tuple where the field element is None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', <torchtext.data.field.Field at 0x136a49e48>),\n",
       " ('labels', None),\n",
       " ('numerical_labels', <torchtext.data.field.Field at 0x1383ca0f0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [(\"text\", TEXT), (\"labels\", None), (\"numerical_labels\", LABEL)]\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TabularDataset` is one of the built-in Datasets in torchtext that handle common data formats; this one is good for .csv files. Other datasets, such as `LanguageModelingDataset` or `TranslationDataset`, are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TabularDataset.splits(\n",
    "    path = folder,\n",
    "    train = 'train_dataset.csv',\n",
    "    validation = 'valid_dataset.csv',\n",
    "    test = 'test_dataset.csv',\n",
    "    format = 'csv',\n",
    "    skip_header = True,\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x13a7f0630> \n",
      "\n",
      "{'text': ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '`', '`', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', '-', 'claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], 'numerical_labels': '2'} \n",
      "\n",
      "dict_keys(['text', 'numerical_labels']) \n",
      "\n",
      "['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '`', '`', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', '-', 'claud', 'van', 'damme', 'or', 'steven', 'segal', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train[0], '\\n')\n",
    "\n",
    "# This is an Example object. The Example object bundles the attributes of a single data \n",
    "# point together. We also see that the text has already been tokenized for us, but has \n",
    "# not yet been converted to integers. \n",
    "\n",
    "print(train[0].__dict__, '\\n')\n",
    "print(train[0].__dict__.keys(), '\\n')\n",
    "print(train[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is constructing the mapping from words to ids. The vocabulary is normally built on the training set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext has its own class called Vocab for handling the vocabulary. The Vocab class holds a mapping from word to id in its `stoi` attribute and a reverse mapping in its `itos` attribute. Words that are not included in the vocabulary will be converted into `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[12])\n",
    "print(TEXT.vocab.stoi['it'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create an Iterator to pass the data to our model.  \n",
    "\n",
    "In torchvision and PyTorch, the processing and batching of data is handled by DataLoaders. For some reason, torchtext has renamed the objects that do the exact same thing to Iterators. The basic functionality is the same, but Iterators, as we will see, have some convenient functionality that is unique to NLP.  \n",
    "\n",
    "We also need to tell the BucketIterator what attribute you want to bucket the data on. In our case, we want to bucket based on the lengths of the comment_text field, so we pass that in as a keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(64, 64, 64),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative would be to use a standard iterator for the test set, given we do not need to shuffle the data since we'll be outputting the predictions at the end of training.  \n",
    "The code would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "    (train, valid),\n",
    "    batch_sizes=(64, 64),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")\n",
    "\n",
    "test_iter = Iterator(\n",
    "    test, batch_size=64, device=device, sort=False, sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a `BucketIterator` looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.iterator.BucketIterator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.text]:[torch.LongTensor of size 10x64]\n",
       "\t[.numerical_labels]:[torch.LongTensor of size 64]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(train_iter))\n",
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = list(train_iter)\n",
    "valid_dataloader = list(valid_iter)\n",
    "test_dataloader = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([29, 64]), torch.Size([64]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = train_dataloader[0]\n",
    "it.text.shape, it.numerical_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dataset': <torchtext.data.dataset.TabularDataset at 0x13a7f0710>,\n",
       " 'fields': dict_keys(['text', 'labels', 'numerical_labels']),\n",
       " 'input_fields': ['text', 'numerical_labels'],\n",
       " 'target_fields': [],\n",
       " 'text': tensor([[2857,    3,    3,  ...,    5, 7999, 2554],\n",
       "         [   0,  135,  230,  ..., 5312,   52, 7164],\n",
       "         [ 856,   38,   86,  ...,   19,   11,   68],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]),\n",
       " 'numerical_labels': tensor([2, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 1, 2, 2, 2, 2, 0, 2, 1, 0, 2,\n",
       "         0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 1, 0, 2, 0, 1, 1, 2, 0, 0, 2, 2,\n",
       "         0, 2, 0, 2, 1, 2, 1, 2, 0, 1, 0, 2, 2, 0, 2, 2])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(list(test_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "<class 'list'>\n",
      "[\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 32x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 35x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 37x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 5x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 34x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 5x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 31x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 3x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 40x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 45x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 8x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 32x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 31x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 36x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 42x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 30x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 34x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 38x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 8x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 8x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 30x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 4x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 33x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 53x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 31x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 42x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.LongTensor of size 53x32]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 32], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 34x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 32x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 4x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 37x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64]]\n"
     ]
    }
   ],
   "source": [
    "iterator = train_dataloader\n",
    "print(len(list(iterator)))\n",
    "print(type(list(iterator)))\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "<class 'torchtext.data.batch.Batch'>\n",
      "torch.Size([13, 64]) torch.Size([64])\n",
      "tensor([[   12,   127,    55,    31,    50,    30,     3,   859,     3,     3,\n",
      "             3,    15,  3063,    12,  3494,    12,    25,     3,    31,   700,\n",
      "           426,     3,  2868,     5,  7718,  1328,    12,  7048,    50,   927,\n",
      "            19,    16,    32,     5,    22,  1000,     5,    29,   463,  1902,\n",
      "            57,  1020,    21,    23,     5,     5,    31,    29,    12,    14,\n",
      "            22,   665,   143,    50,    30,     3,  7830,     3,    21,    12,\n",
      "             3,    12,   963,    22],\n",
      "        [   11,  3029,    10,     7,  4151,   118, 13523,  4235,    20,   234,\n",
      "           165,     5,    41,    11,   110,    11,  1411,  2850,     7,     9,\n",
      "           123, 12018,    10,   408,    69,    14,    59,     6,   763,    48,\n",
      "           681,    28,     3,  3738,  1034,   765,   391,     5,    24,    10,\n",
      "           739,    10,  6898,   121,  1049,  4160,     9,     5,    11,    45,\n",
      "          1376,    54, 10549,  7635,    70,   268,   168,   103, 12267,    11,\n",
      "            17,   123,  2228,   894],\n",
      "        [   22,    48,    26,     3,    51,   105,  1615,     9,    11,    10,\n",
      "            10,  1429,  6273,    75,    25,   509,    58,     7,     3,   982,\n",
      "            27,    29,     5,     4,   353,     5,    26,  9255,    51,   322,\n",
      "          6346,     3,   184,  3299,    75,    72,    68,  2620,    37,    31,\n",
      "             4,    70,    20,     6,  1942,     4,  4225,    75,     5,   401,\n",
      "             4,    23,   227,    11, 10621,   238,  3862,   182,   192,   285,\n",
      "            83,    27,    40,   707],\n",
      "        [ 1255,    24,    31,   220,  2922,    44,     7,    20,  3628,  6258,\n",
      "             5,   908,   874,     4,    92,     8,    33,     3,    63,     6,\n",
      "           636,   497,    96,  1734,   209,   344,   290,  3104,   298,    52,\n",
      "          6975,   317,    24,    68,     4,   769,    15,   539,    12,    17,\n",
      "            16,   804,    10,   503,     8,  4504,   189,   183,   408,    15,\n",
      "          1228,   349,     6,    51,    87,     7,   567,    34,    83,   292,\n",
      "           964,   849,    23,     9],\n",
      "        [  342,    44,   622,    11,  1086,    13,  2446,  1886,  9958,     4,\n",
      "          1368,    20,    13,   552,    86,    70,     3,    81,  7769,  1592,\n",
      "             8,   179,  2754,   314,     3,     7,    13,    77,     8,     7,\n",
      "          2176,     4,   204,    34,   212,   203,  5228,   113,    49,    13,\n",
      "            12,    43,  1877,   744,     3,     6,   299,   275,    20,     5,\n",
      "            20,  3933, 13872,   166,   297,     3,     6,     3,   439,    75,\n",
      "            23,     5,   195,  2640],\n",
      "        [    8,   134,    14,    63,     6,   173,  1023,     4,    10,     3,\n",
      "             4,     4,  1500,     4,    26,  9293,   481,   356,  2036,  2024,\n",
      "          1781,    36,     4,   805,  1830,  5746,     3,  2147,  3643,     5,\n",
      "            78,    24,    26,     5,  2057,     7,  4709,    28,    30,    11,\n",
      "           240,    46,     8,  3065,   388,   212,  1137,     6,    34,    80,\n",
      "            18,   110,  2764,     9,    54,    17,    54,    20,   288,     4,\n",
      "           434,   206,     4,   668],\n",
      "        [ 3517,   126,     3,  3529,   195,     7,     6,    16,    31, 13989,\n",
      "            18,    25,  1146,  1833,   291,    21,    36,    35,  1237,     4,\n",
      "             3,   202,   755,     7,     4,  4349,    17, 14098,    90,   243,\n",
      "           762,   137,   596,   553,   132,  6359,     6,  4091,    10,   488,\n",
      "           338,  6845,    85,   423,     7,  1097,  6525, 13838,   136,     4,\n",
      "             5, 12078,     4, 10683,     8,    38,    10,    10,     4,    16,\n",
      "             6,     7,    15,    18],\n",
      "        [    4,     8,    17,   351,     9,   816,  2362,    12,   445,  1221,\n",
      "          1486,    25,  6121,  3872,     8,    15,    35,   177,     9,     3,\n",
      "           607,    43,     6,     3,    16,   352,    10,     4,    60,  1206,\n",
      "            10,  6711,    16,   339,    40,    19,  2109,    18,  2561,  4689,\n",
      "             3,     5,    24, 13880,     3,   167,     8,  3026,   352,  1761,\n",
      "          2769,     4,    54,    10,    85,  5808,   794,    23,     6,    14,\n",
      "           833,   214,   585,   999],\n",
      "        [   21,    86,    13,     6,  2204,    28,   703,    11,    12,     6,\n",
      "          1682,  4040,    79,     4,   654,     5,  2108,  4760,  1338,    17,\n",
      "          1243,     3,  4691,  1208,    10,  4655,   171,    16,    21,  7922,\n",
      "           139,  2775,   148,   845,    90,   147,    15,     3,     5,     7,\n",
      "           294,    20,   260,     4,  2022,   278,     5,     7,   385,    35,\n",
      "            13,  4502,    71,    36,     7,     4,     8, 12907,    12,    42,\n",
      "           238,     4,   166,   476],\n",
      "        [11870,    18,   940,  2516,  4687,   207,    44,    30,    11,  1253,\n",
      "             6,  1563,   926,     6,    88, 14097,  4592,    21,   105,    10,\n",
      "             6,    52,    78,     7,    26,  1031,    43,    30,   712,   168,\n",
      "             5,    37,    25,    40,    14,     6,    99, 14052,   290,    23,\n",
      "             7,   153,    34,    16,     6,   597,    88,  2296,    38,     5,\n",
      "            83,     6,    10,  1029,    21,  1609,    27,     4,   240,     3,\n",
      "           483,    16,     9,     9],\n",
      "        [    7,  8360,    49,   147,     7,   198,  2924,   125,    47,    36,\n",
      "          6927,    66,   109,   368,   162,  1040,    12,   533,   153,  1034,\n",
      "          3294,    40,  8369,     5,   167,    10,   889,   280,   604,   120,\n",
      "            64,    21,  7857,    23,   959,   606,   185,     7,     7,   651,\n",
      "          1190,   108,  3127,    78,     3,   116,   525,     6,   302,   398,\n",
      "          1048,   944,    12,    43,  1531,     6, 12096,   371,  4276,   608,\n",
      "            56,    59,  3372,   894],\n",
      "        [ 1671,   174,  2621,   872,  1820,   591,  4311,   247,  2384,    47,\n",
      "            73,  8732,  8230,   459,     2,   514,   169,     7,   101,  7981,\n",
      "           221,   670,  1598,  2185,  9331,  1242,   174,   392,   332,  2296,\n",
      "          4507,    20,     2,  5398,  2271,  2948,  1840,  4021,   684,  2401,\n",
      "          7592,    27,  1739,    36,   628,   305,   876,  4845,    16,   670,\n",
      "            58,   390,    55,   190, 10805,  4326,  1506,   150,   151,   983,\n",
      "           142,    26,   979,  4508],\n",
      "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,    33,     2,     2,  8483,     2,     2,\n",
      "             2,     2,     2,     2,   229,     2,     2,     2,     2,     2,\n",
      "             2,     2,    33,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,   107,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2]]) tensor([2, 0, 0, 2, 2, 2, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 2, 2, 0, 0, 2,\n",
      "        2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 0, 2, 1, 2, 2, 0, 0, 0, 2, 0, 0, 2,\n",
      "        2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 1, 0, 0, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = list(train_iter)\n",
    "next_ = train_dataloader[0]\n",
    "print(len(next_))\n",
    "print(type(next_))\n",
    "print(next_.text.shape, next_.numerical_labels.shape)\n",
    "print(next_.text, next_.numerical_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is, finally, our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMultiClass(nn.Module):\n",
    "    def __init__(self, vocab_length, hidden_dim, emb_dim=300, num_lstm_layers=1, num_linear_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_length, emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=num_lstm_layers)\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        for _ in range(num_linear_layers):  # - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.predictor = nn.Linear(hidden_dim, 3)  # 3 possible outcomes: negative, neutral, positive\n",
    "\n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))  # hidden state, cell state (we don't need cell state now)\n",
    "        feature = hdn[-1, :, :]  # taking the last output from the LSTM (we're dealing with many-to-one problem)\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "            feature = self.sigm(feature)\n",
    "         \n",
    "        preds = self.predictor(feature)  # outside for loop\n",
    "        preds = self.tanh(preds)\n",
    "        return preds\n",
    "\n",
    "\n",
    "model = LSTMultiClass(vocab_length=len(TEXT.vocab), hidden_dim=500, emb_dim=100, num_linear_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:47<00:00,  2.80it/s]\n",
      "INFO:__main__:Epoch: 1, Training Loss: 0.4403, Validation Loss: 0.5091\n",
      "100%|██████████| 134/134 [00:48<00:00,  2.77it/s]\n",
      "INFO:__main__:Epoch: 2, Training Loss: 0.4404, Validation Loss: 0.5091\n",
      "100%|██████████| 134/134 [00:48<00:00,  2.78it/s]\n",
      "INFO:__main__:Epoch: 3, Training Loss: 0.4404, Validation Loss: 0.5091\n"
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "epochs = 3\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    # running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for batch in tqdm(train_dataloader):  # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        x = batch.text  # independent variable (the input to the model)\n",
    "        y = batch.numerical_labels.unsqueeze(1).float()  # dependent variable (the supervision data)\n",
    "        opt.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = loss_func(predictions, y[:,0].long()) #(y.long(), predictions.long())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.data * x.size(0)\n",
    "    epoch_loss = running_loss / len(train)\n",
    "\n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            x = batch.text\n",
    "            y = batch.numerical_labels.unsqueeze(1).float()\n",
    "            predictions = model(x)\n",
    "            loss = loss_func(predictions, y[:,0].long())  # loss_func(y, predictions)\n",
    "            val_loss += loss.data * x.size(0)\n",
    "\n",
    "    val_loss /= len(valid)\n",
    "    logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Dataset (multiple labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
       "      <td>0005300084f90edc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bbq \\n\\nbe a man and lets discuss it-maybe ove...</td>\n",
       "      <td>00054a5e18b50dd4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>0006f16e4e9f292e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oh, and the girl above started her arguments w...</td>\n",
       "      <td>00070ef96486d6f9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...</td>\n",
       "      <td>00078f8ce7eb276d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski</td>\n",
       "      <td>000897889268bc93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Mitsurugi point made no sense - why not ar...</td>\n",
       "      <td>0009801bd85e5806</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Don't mean to bother you \\n\\nI see that you're...</td>\n",
       "      <td>0009eaea3325de8c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"\\n\\n Regarding your recent edits \\n\\nOnce aga...</td>\n",
       "      <td>000b08c464718505</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"\\nGood to know. About me, yeah, I'm studying ...</td>\n",
       "      <td>000bfd0867774845</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"\\n\\n Snowflakes are NOT always symmetrical! \\...</td>\n",
       "      <td>000c0dfd995809fa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...</td>\n",
       "      <td>000c6a3f0cd3ba8e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"\\n\\nRe-considering 1st paragraph edit?\\nI don...</td>\n",
       "      <td>000cfee90f50d471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment_text                id  \\\n",
       "0   Explanation\\nWhy the edits made under my usern...  0000997932d777bf   \n",
       "1   D'aww! He matches this background colour I'm s...  000103f0d9cfb60f   \n",
       "2   Hey man, I'm really not trying to edit war. It...  000113f07ec002fd   \n",
       "3   \"\\nMore\\nI can't make any real suggestions on ...  0001b41b1c6bb37e   \n",
       "4   You, sir, are my hero. Any chance you remember...  0001d958c54c6e35   \n",
       "5   \"\\n\\nCongratulations from me as well, use the ...  00025465d4725e87   \n",
       "6        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK  0002bcb3da6cb337   \n",
       "7   Your vandalism to the Matt Shirvington article...  00031b1e95af7921   \n",
       "8   Sorry if the word 'nonsense' was offensive to ...  00037261f536c51d   \n",
       "9   alignment on this subject and which are contra...  00040093b2687caa   \n",
       "10  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...  0005300084f90edc   \n",
       "11  bbq \\n\\nbe a man and lets discuss it-maybe ove...  00054a5e18b50dd4   \n",
       "12  Hey... what is it..\\n@ | talk .\\nWhat is it......  0005c987bdfc9d4b   \n",
       "13  Before you start throwing accusations and warn...  0006f16e4e9f292e   \n",
       "14  Oh, and the girl above started her arguments w...  00070ef96486d6f9   \n",
       "15  \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...  00078f8ce7eb276d   \n",
       "16  Bye! \\n\\nDon't look, come or think of comming ...  0007e25b2121310b   \n",
       "17   REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski  000897889268bc93   \n",
       "18  The Mitsurugi point made no sense - why not ar...  0009801bd85e5806   \n",
       "19  Don't mean to bother you \\n\\nI see that you're...  0009eaea3325de8c   \n",
       "20  \"\\n\\n Regarding your recent edits \\n\\nOnce aga...  000b08c464718505   \n",
       "21  \"\\nGood to know. About me, yeah, I'm studying ...  000bfd0867774845   \n",
       "22  \"\\n\\n Snowflakes are NOT always symmetrical! \\...  000c0dfd995809fa   \n",
       "23  \"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...  000c6a3f0cd3ba8e   \n",
       "24  \"\\n\\nRe-considering 1st paragraph edit?\\nI don...  000cfee90f50d471   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0       0             0        0       0       0              0  \n",
       "1       0             0        0       0       0              0  \n",
       "2       0             0        0       0       0              0  \n",
       "3       0             0        0       0       0              0  \n",
       "4       0             0        0       0       0              0  \n",
       "5       0             0        0       0       0              0  \n",
       "6       1             1        1       0       1              0  \n",
       "7       0             0        0       0       0              0  \n",
       "8       0             0        0       0       0              0  \n",
       "9       0             0        0       0       0              0  \n",
       "10      0             0        0       0       0              0  \n",
       "11      0             0        0       0       0              0  \n",
       "12      1             0        0       0       0              0  \n",
       "13      0             0        0       0       0              0  \n",
       "14      0             0        0       0       0              0  \n",
       "15      0             0        0       0       0              0  \n",
       "16      1             0        0       0       0              0  \n",
       "17      0             0        0       0       0              0  \n",
       "18      0             0        0       0       0              0  \n",
       "19      0             0        0       0       0              0  \n",
       "20      0             0        0       0       0              0  \n",
       "21      0             0        0       0       0              0  \n",
       "22      0             0        0       0       0              0  \n",
       "23      0             0        0       0       0              0  \n",
       "24      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = os.path.join(os.getenv('DATA_DIR'), 'toxic_comments_dataset')\n",
    "train_dataset = pd.read_csv(os.path.join(folder, 'train_dataset.csv'))\n",
    "valid_dataset = pd.read_csv(os.path.join(folder, 'valid_dataset.csv'))\n",
    "test_dataset = pd.read_csv(os.path.join(folder, 'test_dataset.csv'))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize='spacy', lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comment_text', <torchtext.data.field.Field at 0x14027cb00>),\n",
       " ('id', None),\n",
       " ('toxic', <torchtext.data.field.Field at 0x1401d8e48>),\n",
       " ('severe_toxic', <torchtext.data.field.Field at 0x1401d8e48>),\n",
       " ('obscene', <torchtext.data.field.Field at 0x1401d8e48>),\n",
       " ('threat', <torchtext.data.field.Field at 0x1401d8e48>),\n",
       " ('insult', <torchtext.data.field.Field at 0x1401d8e48>),\n",
       " ('identity_hate', <torchtext.data.field.Field at 0x1401d8e48>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [\n",
    "    (\"comment_text\", TEXT),\n",
    "    (\"id\", None),  \n",
    "    (\"toxic\", LABEL), \n",
    "    (\"severe_toxic\", LABEL),\n",
    "    (\"obscene\", LABEL),\n",
    "    (\"threat\", LABEL),\n",
    "    (\"insult\", LABEL),\n",
    "    (\"identity_hate\", LABEL)\n",
    "]\n",
    "\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TabularDataset.splits(\n",
    "    path = folder,\n",
    "    train = 'train_dataset.csv',\n",
    "    validation = 'valid_dataset.csv',\n",
    "    test = 'test_dataset.csv',\n",
    "    format = 'csv',\n",
    "    skip_header = True,\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(7, 7, 7),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.comment_text),\n",
    "    sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 4,\n",
       " 'dataset': <torchtext.data.dataset.TabularDataset at 0x140132710>,\n",
       " 'fields': dict_keys(['comment_text', 'id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']),\n",
       " 'input_fields': ['comment_text',\n",
       "  'toxic',\n",
       "  'severe_toxic',\n",
       "  'obscene',\n",
       "  'threat',\n",
       "  'insult',\n",
       "  'identity_hate'],\n",
       " 'target_fields': [],\n",
       " 'comment_text': tensor([[  5,   5,  88,   5],\n",
       "         [ 34,  15,   7,  15],\n",
       "         [ 36, 209,   0,  93],\n",
       "         ...,\n",
       "         [108,   1,   1,   1],\n",
       "         [ 39,   1,   1,   1],\n",
       "         [  5,   1,   1,   1]]),\n",
       " 'toxic': tensor([0, 0, 0, 0]),\n",
       " 'severe_toxic': tensor([0, 0, 0, 0]),\n",
       " 'obscene': tensor([0, 0, 0, 0]),\n",
       " 'threat': tensor([0, 0, 0, 0]),\n",
       " 'insult': tensor([0, 0, 0, 0]),\n",
       " 'identity_hate': tensor([0, 0, 0, 0])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(list(train_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = list(train_iter)\n",
    "valid_dataloader = list(valid_iter)\n",
    "#test_dataloader = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTMBinary(nn.Module):\n",
    "    def __init__(self, vocab_length, hidden_dim, emb_dim=300, num_lstm_layers=1, num_linear_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_linear_layers = num_linear_layers\n",
    "        self.embedding = nn.Embedding(vocab_length, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers = num_lstm_layers)\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_linear_layers)]\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_dim, 6)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        preds = self.embedding(seq)\n",
    "        preds, _ = self.lstm(preds)\n",
    "        preds = preds[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            preds = self.sigm(layer(preds))\n",
    "        preds = self.output(preds)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "model = LSTMBinary(vocab_length=len(TEXT.vocab), hidden_dim=500, emb_dim=100, num_linear_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:09<00:00,  2.36s/it]\n",
      "INFO:__main__:Epoch: 1, Training Loss: 5.1319, Validation Loss: 6.2896\n",
      "100%|██████████| 4/4 [00:10<00:00,  2.61s/it]\n",
      "INFO:__main__:Epoch: 2, Training Loss: 1.4135, Validation Loss: 5.7671\n",
      "100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n",
      "INFO:__main__:Epoch: 3, Training Loss: 1.6356, Validation Loss: 5.7868\n"
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 3\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        x = batch.comment_text\n",
    "        y = torch.cat([getattr(batch, label).unsqueeze(1) for label in labels], dim=1).float()\n",
    "        opt.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = loss_func(predictions, y)\n",
    "        loss.backward()\n",
    "        opt.step()     \n",
    "        running_loss += loss.data * x.size(0)\n",
    "    epoch_loss = running_loss / len(train)\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            x = batch.comment_text\n",
    "            y = torch.cat([getattr(batch, label).unsqueeze(1) for label in labels], dim=1).float()\n",
    "            predictions = model(x)\n",
    "            loss = loss_func(predictions, y)\n",
    "            valid_loss += loss.data * x.size(0)\n",
    "\n",
    "        valid_loss /= len(valid)\n",
    "    logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " [torchtext.data.batch.Batch of size 7]\n",
       " \t[.comment_text]:[torch.LongTensor of size 48x7]\n",
       " \t[.toxic]:[torch.LongTensor of size 7]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 7]\n",
       " \t[.obscene]:[torch.LongTensor of size 7]\n",
       " \t[.threat]:[torch.LongTensor of size 7]\n",
       " \t[.insult]:[torch.LongTensor of size 7]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 7], \n",
       " [torchtext.data.batch.Batch of size 7]\n",
       " \t[.comment_text]:[torch.LongTensor of size 133x7]\n",
       " \t[.toxic]:[torch.LongTensor of size 7]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 7]\n",
       " \t[.obscene]:[torch.LongTensor of size 7]\n",
       " \t[.threat]:[torch.LongTensor of size 7]\n",
       " \t[.insult]:[torch.LongTensor of size 7]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 7], \n",
       " [torchtext.data.batch.Batch of size 4]\n",
       " \t[.comment_text]:[torch.LongTensor of size 587x4]\n",
       " \t[.toxic]:[torch.LongTensor of size 4]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 4]\n",
       " \t[.obscene]:[torch.LongTensor of size 4]\n",
       " \t[.threat]:[torch.LongTensor of size 4]\n",
       " \t[.insult]:[torch.LongTensor of size 4]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 4], \n",
       " [torchtext.data.batch.Batch of size 7]\n",
       " \t[.comment_text]:[torch.LongTensor of size 17x7]\n",
       " \t[.toxic]:[torch.LongTensor of size 7]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 7]\n",
       " \t[.obscene]:[torch.LongTensor of size 7]\n",
       " \t[.threat]:[torch.LongTensor of size 7]\n",
       " \t[.insult]:[torch.LongTensor of size 7]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 7]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = list(train_iter)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([587, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[2].comment_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "hey man , i 'm really not <unk> to edit <unk> . it 's just that this <unk> is <unk> removing relevant information and <unk> to me <unk> edits <unk> of my talk page . he <unk> to <unk> more about the formatting than the <unk> info .\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "oh , and the <unk> <unk> <unk> her <unk> with me . she stuck her <unk> where it does n't belong . i believe the argument was <unk> me and <unk> . but like i <unk> , the <unk> was <unk> and i <unk> . thanks , <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "<unk> ! he <unk> this <unk> <unk> i 'm <unk> stuck with . thanks .   ( talk ) <unk> , <unk> <unk> , <unk> ( <unk> ) <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "your vandalism to the <unk> <unk> article has been reverted .   please do n't do it again , or you will be <unk> . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "the <unk> point made no <unk> - why not <unk> to include <unk> on <unk> <unk> 's page to include more information ? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  the signpost : <unk> <unk> <unk> \n",
      "\n",
      "  read this signpost in full \n",
      "  single - page \n",
      "  <unk> <unk> \" <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "you , <unk> , are my <unk> . any <unk> you <unk> what page that 's on ? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(l[0].comment_text[1])):\n",
    "    print(\"\\n NEW SENTENCE \\n\")\n",
    "    print(' '.join([TEXT.vocab.itos[x] for x in l[0].comment_text[:,i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
