{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext tutorial\n",
    "\n",
    "\n",
    "Made by [Andrea Sottana](http://github.com/AndreaSottana) and adapted from the following [tutorial](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)\n",
    "\n",
    "This notebook shows how you can use your own custom dataset starting from the raw form (for example a .csv file) with the `torch` library, to easily do some cleaning and preprocessing (such as tokenization), build a vocabulary, and ultimately create an `Iterator` or `BucketIterator` object that you can feed it into a neural network built using the `torch` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset, Iterator, BucketIterator\n",
    "from torchtext import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=20)  # shows info level loggings (we'll use this to print the loss during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: multi-class classification, single label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data `Field` specifies how you want a certain field to be processed, for example whether you want to tokenize it, lower case the text etc. In our case we have one text field, and one label field for the sentiment analysis, which we represent with an integer, where 0 is negative, 1 is neutral and 2 is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize='spacy', lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>numerical_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>No surprises .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels  \\\n",
       "0     The Rock is destined to be the 21st Century 's...  positive   \n",
       "1     The gorgeously elaborate continuation of `` Th...  positive   \n",
       "2     Singer\\/composer Bryan Adams contributes a sle...  positive   \n",
       "3     You 'd think by now America would have had eno...   neutral   \n",
       "4                  Yet the act is still charming here .  positive   \n",
       "...                                                 ...       ...   \n",
       "8539                                    A real snooze .  negative   \n",
       "8540                                     No surprises .  negative   \n",
       "8541  We 've seen the hippie-turned-yuppie plot befo...  positive   \n",
       "8542  Her fans walked out muttering words like `` ho...  negative   \n",
       "8543                                In this case zero .  negative   \n",
       "\n",
       "      numerical_labels  \n",
       "0                    2  \n",
       "1                    2  \n",
       "2                    2  \n",
       "3                    1  \n",
       "4                    2  \n",
       "...                ...  \n",
       "8539                 0  \n",
       "8540                 0  \n",
       "8541                 2  \n",
       "8542                 0  \n",
       "8543                 0  \n",
       "\n",
       "[8544 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember to source the .envrc file in the terminal before launching this notebook to \n",
    "# ensure it can use the environment variables correctly.\n",
    "\n",
    "folder = os.path.join(os.getenv('DATA_DIR'), 'movie_review_dataset')\n",
    "train_dataset = pd.read_csv(os.path.join(folder, 'train_dataset.csv'))\n",
    "valid_dataset = pd.read_csv(os.path.join(folder, 'valid_dataset.csv'))\n",
    "test_dataset = pd.read_csv(os.path.join(folder, 'test_dataset.csv'))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells the field what data to work on. Note that the fields we pass in must be in the same order as the columns in our csv file. For the columns we don't use, we pass in a tuple where the field element is None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', <torchtext.data.field.Field at 0x13614ad68>),\n",
       " ('labels', None),\n",
       " ('numerical_labels', <torchtext.data.field.Field at 0x137ce70b8>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [(\"text\", TEXT), (\"labels\", None), (\"numerical_labels\", LABEL)]\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TabularDataset` is one of the built-in Datasets in torchtext that handle common data formats; this one is good for .csv files. Other datasets, such as `LanguageModelingDataset` or `TranslationDataset`, are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TabularDataset.splits(\n",
    "    path = folder,\n",
    "    train = 'train_dataset.csv',\n",
    "    validation = 'valid_dataset.csv',\n",
    "    test = 'test_dataset.csv',\n",
    "    format = 'csv',\n",
    "    skip_header = True,\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x139ffd5c0> \n",
      "\n",
      "{'text': ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '`', '`', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', '-', 'claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], 'numerical_labels': '2'} \n",
      "\n",
      "dict_keys(['text', 'numerical_labels']) \n",
      "\n",
      "['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '`', '`', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', '-', 'claud', 'van', 'damme', 'or', 'steven', 'segal', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train[0], '\\n')\n",
    "\n",
    "# This is an Example object. The Example object bundles the attributes of a single data \n",
    "# point together. We also see that the text has already been tokenized for us, but has \n",
    "# not yet been converted to integers. \n",
    "\n",
    "print(train[0].__dict__, '\\n')\n",
    "print(train[0].__dict__.keys(), '\\n')\n",
    "print(train[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is constructing the mapping from words to ids. The vocabulary is normally built on the training set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext has its own class called Vocab for handling the vocabulary. The Vocab class holds a mapping from word to id in its `stoi` attribute and a reverse mapping in its `itos` attribute. Words that are not included in the vocabulary will be converted into `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[12])\n",
    "print(TEXT.vocab.stoi['it'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create an Iterator to pass the data to our model.  \n",
    "\n",
    "In torchvision and PyTorch, the processing and batching of data is handled by DataLoaders. For some reason, torchtext has renamed the objects that do the exact same thing to Iterators. The basic functionality is the same, but Iterators, as we will see, have some convenient functionality that is unique to NLP.  \n",
    "\n",
    "We also need to tell the BucketIterator what attribute you want to bucket the data on. In our case, we want to bucket based on the lengths of the comment_text field, so we pass that in as a keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(64, 64, 64),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative would be to use a standard iterator for the test set, given we do not need to shuffle the data since we'll be outputting the predictions at the end of training.  \n",
    "The code would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "    (train, valid),\n",
    "    batch_sizes=(64, 64),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")\n",
    "\n",
    "test_iter = Iterator(\n",
    "    test, batch_size=64, device=device, sort=False, sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a `BucketIterator` looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.iterator.BucketIterator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.text]:[torch.LongTensor of size 26x64]\n",
       "\t[.numerical_labels]:[torch.LongTensor of size 64]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(train_iter))\n",
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = list(train_iter)\n",
    "valid_dataloader = list(valid_iter)\n",
    "test_dataloader = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 64]), torch.Size([64]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = train_dataloader[0]\n",
    "it.text.shape, it.numerical_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dataset': <torchtext.data.dataset.TabularDataset at 0x139ffd710>,\n",
       " 'fields': dict_keys(['text', 'labels', 'numerical_labels']),\n",
       " 'input_fields': ['text', 'numerical_labels'],\n",
       " 'target_fields': [],\n",
       " 'text': tensor([[ 106,    5,    3,  ...,   16,  138,  436],\n",
       "         [1790, 4857,  147,  ...,   54, 4456,    5],\n",
       "         [  50,    4,   10,  ...,   34, 4359, 2691],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]),\n",
       " 'numerical_labels': tensor([0, 0, 0, 0, 1, 1, 0, 0, 2, 2, 2, 0, 2, 1, 0, 0, 0, 2, 2, 0, 0, 1, 2, 0,\n",
       "         2, 2, 0, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 2, 1, 2, 0, 0, 1, 0, 0, 0, 2, 0,\n",
       "         1, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 1, 0, 0])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(list(test_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "<class 'list'>\n",
      "[\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 8x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 36x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 35x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 32x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 39x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 44x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 37x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 38x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 27x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 30x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 31x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 33x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 5x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 32x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 33x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 4x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 30x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 34x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 31x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 5x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 8x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 3x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 53x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 41x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 13x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.LongTensor of size 51x32]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 32], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 18x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 23x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 10x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 38x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 11x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 28x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 6x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 24x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 21x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 33x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 31x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 4x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 25x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 8x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 44x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 30x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 12x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 9x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 35x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 16x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 17x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 26x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 14x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 29x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 20x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 15x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 19x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64], \n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 7x64]\n",
      "\t[.numerical_labels]:[torch.LongTensor of size 64]]\n"
     ]
    }
   ],
   "source": [
    "iterator = train_dataloader\n",
    "print(len(list(iterator)))\n",
    "print(type(list(iterator)))\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "<class 'torchtext.data.batch.Batch'>\n",
      "torch.Size([10, 64]) torch.Size([64])\n",
      "tensor([[   12,    54,  3448,    12,  3975,     3,     5,   883,    16,    55,\n",
      "            72,    32,     5,   763,   542,  3114,     3,     3,    12,    22,\n",
      "             3,  7671,    12,     5,    56,    55,     6,  6724,    23,    24,\n",
      "            15,    54,    16,     5,    12,  2096,  2339,  7945,    76,  1278,\n",
      "             3,   701,    54,    12,   683,    22,    46,    29,     5,     8,\n",
      "         12173,    14,    12,   151,     6,     5,   390,    30,    50,  3259,\n",
      "            36, 12647,    31,    25],\n",
      "        [ 9821,    11,  4078,    11,    11,   740,  2316,     4,    12,    11,\n",
      "           105,   409,   471,    11,     6,   700,   368,    17,    11,  4950,\n",
      "           121,  7341,    11,  2791,  9825,    11,  2311,  3334,   738,    62,\n",
      "          1213,    98,    12, 10711,   330,     4,    74,  5101,    35,  7395,\n",
      "            20,     4,    98,    11,    41,  4947,    33,  2287,  7971,  8985,\n",
      "           236,   131, 14818,     3,   136,  6782,     6,     8,  6983,    11,\n",
      "             7,     4,     7,    25],\n",
      "        [   42,   470,    10,     5,   170,   230,   442,   166,    59,   465,\n",
      "          1610,  3304,     4,  1385, 14597,     9,  6096,   444,    62, 15368,\n",
      "          7126,     4,  8888,  8235,    32,     5,    10,  4023,     4,   297,\n",
      "            15,    12,    11,     9,   169,  1137,   477,   672,     5,   548,\n",
      "            10,    16,     3,  2885,     5,    80,   396,    88,   190,   209,\n",
      "         13670,   760,     3,    97,    85,   194,    75,  1403,  3537,     5,\n",
      "             5,   246,     3,  4832],\n",
      "        [  326,    10,     4,    88,    10,   997,    18,     9,    26,    89,\n",
      "           267,   295,    16,     6,    70,   274,  5019,  8108,     5,   399,\n",
      "            41,    24,    32,     7,   264,   206,    26,    22,   552,   115,\n",
      "             5,   226,  6293,  6605,   378,     6,    32,  7492,   144,   605,\n",
      "            34,   226,    20,  3462,   492,     4,  3180,   177,  5470,   233,\n",
      "          5396,     4,    52,   238,   157,     7,    50,     5,     4,   295,\n",
      "           722,    11,    63,  1513],\n",
      "        [    4,     5,  2545,  3472,     5,    64,  1332,  1361,   291,    56,\n",
      "            15,     9,    12,    63,   104,     4,  5893,  3179,    78,     7,\n",
      "          1109,  4085,  2382,   131,    29,     8,     3,   563,    52,  1005,\n",
      "          2630,   219,     6,     6,     4,    36,  3754,     3,   150,     6,\n",
      "            15,   519,   258,    16,   165,  5095,     3,   341,     7,    92,\n",
      "             6,    12,    12,    42,     7,   407,    16,  7706,    16,   402,\n",
      "          9796,   117,  2068,     4],\n",
      "        [   83,   518,     4,   565,   722,   899,   234,    16,  2045,   118,\n",
      "            84,  3540,    11,  5048,    12,   162,    10,    39,    56,     3,\n",
      "             8,  7230,   587,     4,    16,  1063,   490,   728,  4644,    10,\n",
      "            41,    10,  1817,  7389,  1537,    43,  2734, 10422,     6,   145,\n",
      "           555,    19,    10,   145,     6,    35,    80,     6, 14207,  1763,\n",
      "           196,    11,   932,   184,   207,  8139,   212,     4,    12,     4,\n",
      "            43,   320,  1383,    66],\n",
      "        [ 6668,  7852,    64,     6,     9,    49,     6,   163,     7,     7,\n",
      "           435,     6,     5,    20,  1966,     9,   585,     6,   166,   356,\n",
      "          1553,  2254,     6,   126,  4278,   873,     8,    68,    60,   272,\n",
      "             5,  5276,   502,  4342,  1078,     5,     6,   110,     5,     3,\n",
      "            15,  5799,    23,   192,     5,    67,    10,    70,   752,  2210,\n",
      "           218,  8808,    53,    14,    19,     6,   337,  4589,    11,    13,\n",
      "             5,    12,    14,    12],\n",
      "        [   67,     7,  1594,  1913,  6296,   351,  2240,   494,   109,   198,\n",
      "          3086, 10088,   934,    14,     8,  1853,    16,  1032,     9,   893,\n",
      "             6,    22,  2854,   539,    61,   211,  1428,     7,    82,     8,\n",
      "         15382,    11,     3,   197,    64,    78,  3387,  4922,   144,   231,\n",
      "            13,    11,  2197,  1154,   324,     8,     5,   171,     9,   105,\n",
      "            19,    43,     8,   792,  3998,  4750,    51,   412,   554,    11,\n",
      "           821,    21,   517,    11],\n",
      "        [ 2769,  2747,   763,    20,  3386,  1353,   590,  2166,   761,   133,\n",
      "          2093,   726,   471,   244,  1433,   189,  1083,   642,  2837,   527,\n",
      "           218,  8642,   294,   105,  1691,  2744,    12,  4762,   195,  1388,\n",
      "          7870,   170,  6723,  2625,  5772,   581,  3887,     3,   340,    68,\n",
      "          1924,   170,  3427,  1234,   640,  2079, 12105,   940,   736,   892,\n",
      "            36,    96,   630,  2839,  4223,   863,    20,  7029,     2,    42,\n",
      "            94,    71,   980,    30],\n",
      "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,    29,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,  1113,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,    29,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,    51,     2,\n",
      "             2,     2,     2,     2]]) tensor([0, 1, 0, 2, 2, 0, 0, 2, 1, 0, 1, 0, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 0, 0,\n",
      "        2, 2, 0, 1, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 1, 2, 2, 0, 0, 2, 0,\n",
      "        2, 0, 1, 0, 2, 0, 0, 2, 1, 1, 1, 0, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = list(train_iter)\n",
    "next_ = train_dataloader[0]\n",
    "print(len(next_))\n",
    "print(type(next_))\n",
    "print(next_.text.shape, next_.numerical_labels.shape)\n",
    "print(next_.text, next_.numerical_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is, finally, our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMultiClass(nn.Module):\n",
    "    def __init__(self, vocab_length, hidden_dim, emb_dim=300, num_lstm_layers=1, num_linear_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_length, emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=num_lstm_layers)\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_linear_layers)]\n",
    "        )\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.predictor = nn.Linear(hidden_dim, 3)  # 3 possible outcomes: negative, neutral, positive\n",
    "\n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))  # hidden state, cell state (we don't need cell state now)\n",
    "        feature = hdn[-1, :, :]  # taking the last output from the LSTM (we're dealing with many-to-one problem)\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "            feature = self.sigm(feature)\n",
    "         \n",
    "        preds = self.predictor(feature)\n",
    "        preds = self.tanh(preds)\n",
    "        return preds\n",
    "\n",
    "\n",
    "model = LSTMultiClass(vocab_length=len(TEXT.vocab), hidden_dim=500, emb_dim=100, num_linear_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:43<00:00,  3.08it/s]\n",
      "INFO:__main__:Epoch: 1, Training Loss: 0.4888, Validation Loss: 0.5559\n",
      "100%|██████████| 134/134 [00:48<00:00,  2.77it/s]\n",
      "INFO:__main__:Epoch: 2, Training Loss: 0.4892, Validation Loss: 0.5559\n",
      "100%|██████████| 134/134 [00:42<00:00,  3.13it/s]\n",
      "INFO:__main__:Epoch: 3, Training Loss: 0.4892, Validation Loss: 0.5559\n"
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "epochs = 3\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    model.train() # turn on training mode\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        x = batch.text  # independent variable (the input to the model)\n",
    "        y = batch.numerical_labels.unsqueeze(1).float()  # dependent variable (the supervision data)\n",
    "        opt.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = loss_func(predictions, y[:,0].long()) \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.data * x.size(0)\n",
    "    epoch_loss = running_loss / len(train)\n",
    "\n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            x = batch.text\n",
    "            y = batch.numerical_labels.unsqueeze(1).float()\n",
    "            predictions = model(x)\n",
    "            loss = loss_func(predictions, y[:,0].long())  \n",
    "            val_loss += loss.data * x.size(0)\n",
    "\n",
    "    val_loss /= len(valid)\n",
    "    logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: binary classification, multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
       "      <td>0005300084f90edc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bbq \\n\\nbe a man and lets discuss it-maybe ove...</td>\n",
       "      <td>00054a5e18b50dd4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>0006f16e4e9f292e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oh, and the girl above started her arguments w...</td>\n",
       "      <td>00070ef96486d6f9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...</td>\n",
       "      <td>00078f8ce7eb276d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski</td>\n",
       "      <td>000897889268bc93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Mitsurugi point made no sense - why not ar...</td>\n",
       "      <td>0009801bd85e5806</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Don't mean to bother you \\n\\nI see that you're...</td>\n",
       "      <td>0009eaea3325de8c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"\\n\\n Regarding your recent edits \\n\\nOnce aga...</td>\n",
       "      <td>000b08c464718505</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"\\nGood to know. About me, yeah, I'm studying ...</td>\n",
       "      <td>000bfd0867774845</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"\\n\\n Snowflakes are NOT always symmetrical! \\...</td>\n",
       "      <td>000c0dfd995809fa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...</td>\n",
       "      <td>000c6a3f0cd3ba8e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"\\n\\nRe-considering 1st paragraph edit?\\nI don...</td>\n",
       "      <td>000cfee90f50d471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment_text                id  \\\n",
       "0   Explanation\\nWhy the edits made under my usern...  0000997932d777bf   \n",
       "1   D'aww! He matches this background colour I'm s...  000103f0d9cfb60f   \n",
       "2   Hey man, I'm really not trying to edit war. It...  000113f07ec002fd   \n",
       "3   \"\\nMore\\nI can't make any real suggestions on ...  0001b41b1c6bb37e   \n",
       "4   You, sir, are my hero. Any chance you remember...  0001d958c54c6e35   \n",
       "5   \"\\n\\nCongratulations from me as well, use the ...  00025465d4725e87   \n",
       "6        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK  0002bcb3da6cb337   \n",
       "7   Your vandalism to the Matt Shirvington article...  00031b1e95af7921   \n",
       "8   Sorry if the word 'nonsense' was offensive to ...  00037261f536c51d   \n",
       "9   alignment on this subject and which are contra...  00040093b2687caa   \n",
       "10  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...  0005300084f90edc   \n",
       "11  bbq \\n\\nbe a man and lets discuss it-maybe ove...  00054a5e18b50dd4   \n",
       "12  Hey... what is it..\\n@ | talk .\\nWhat is it......  0005c987bdfc9d4b   \n",
       "13  Before you start throwing accusations and warn...  0006f16e4e9f292e   \n",
       "14  Oh, and the girl above started her arguments w...  00070ef96486d6f9   \n",
       "15  \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...  00078f8ce7eb276d   \n",
       "16  Bye! \\n\\nDon't look, come or think of comming ...  0007e25b2121310b   \n",
       "17   REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski  000897889268bc93   \n",
       "18  The Mitsurugi point made no sense - why not ar...  0009801bd85e5806   \n",
       "19  Don't mean to bother you \\n\\nI see that you're...  0009eaea3325de8c   \n",
       "20  \"\\n\\n Regarding your recent edits \\n\\nOnce aga...  000b08c464718505   \n",
       "21  \"\\nGood to know. About me, yeah, I'm studying ...  000bfd0867774845   \n",
       "22  \"\\n\\n Snowflakes are NOT always symmetrical! \\...  000c0dfd995809fa   \n",
       "23  \"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...  000c6a3f0cd3ba8e   \n",
       "24  \"\\n\\nRe-considering 1st paragraph edit?\\nI don...  000cfee90f50d471   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0       0             0        0       0       0              0  \n",
       "1       0             0        0       0       0              0  \n",
       "2       0             0        0       0       0              0  \n",
       "3       0             0        0       0       0              0  \n",
       "4       0             0        0       0       0              0  \n",
       "5       0             0        0       0       0              0  \n",
       "6       1             1        1       0       1              0  \n",
       "7       0             0        0       0       0              0  \n",
       "8       0             0        0       0       0              0  \n",
       "9       0             0        0       0       0              0  \n",
       "10      0             0        0       0       0              0  \n",
       "11      0             0        0       0       0              0  \n",
       "12      1             0        0       0       0              0  \n",
       "13      0             0        0       0       0              0  \n",
       "14      0             0        0       0       0              0  \n",
       "15      0             0        0       0       0              0  \n",
       "16      1             0        0       0       0              0  \n",
       "17      0             0        0       0       0              0  \n",
       "18      0             0        0       0       0              0  \n",
       "19      0             0        0       0       0              0  \n",
       "20      0             0        0       0       0              0  \n",
       "21      0             0        0       0       0              0  \n",
       "22      0             0        0       0       0              0  \n",
       "23      0             0        0       0       0              0  \n",
       "24      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = os.path.join(os.getenv('DATA_DIR'), 'toxic_comments_dataset')\n",
    "train_dataset = pd.read_csv(os.path.join(folder, 'train_dataset.csv'))\n",
    "valid_dataset = pd.read_csv(os.path.join(folder, 'valid_dataset.csv'))\n",
    "test_dataset = pd.read_csv(os.path.join(folder, 'test_dataset.csv'))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize='spacy', lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comment_text', <torchtext.data.field.Field at 0x13f90b668>),\n",
       " ('id', None),\n",
       " ('toxic', <torchtext.data.field.Field at 0x13f91dd30>),\n",
       " ('severe_toxic', <torchtext.data.field.Field at 0x13f91dd30>),\n",
       " ('obscene', <torchtext.data.field.Field at 0x13f91dd30>),\n",
       " ('threat', <torchtext.data.field.Field at 0x13f91dd30>),\n",
       " ('insult', <torchtext.data.field.Field at 0x13f91dd30>),\n",
       " ('identity_hate', <torchtext.data.field.Field at 0x13f91dd30>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [\n",
    "    (\"comment_text\", TEXT),\n",
    "    (\"id\", None),  \n",
    "    (\"toxic\", LABEL), \n",
    "    (\"severe_toxic\", LABEL),\n",
    "    (\"obscene\", LABEL),\n",
    "    (\"threat\", LABEL),\n",
    "    (\"insult\", LABEL),\n",
    "    (\"identity_hate\", LABEL)\n",
    "]\n",
    "\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TabularDataset.splits(\n",
    "    path = folder,\n",
    "    train = 'train_dataset.csv',\n",
    "    validation = 'valid_dataset.csv',\n",
    "    test = 'test_dataset.csv',\n",
    "    format = 'csv',\n",
    "    skip_header = True,\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(7, 7, 7),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.comment_text),\n",
    "    sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 4,\n",
       " 'dataset': <torchtext.data.dataset.TabularDataset at 0x13f6522b0>,\n",
       " 'fields': dict_keys(['comment_text', 'id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']),\n",
       " 'input_fields': ['comment_text',\n",
       "  'toxic',\n",
       "  'severe_toxic',\n",
       "  'obscene',\n",
       "  'threat',\n",
       "  'insult',\n",
       "  'identity_hate'],\n",
       " 'target_fields': [],\n",
       " 'comment_text': tensor([[  5,   5,  88,   5],\n",
       "         [ 34,  15,   7,  15],\n",
       "         [ 36, 209,   0,  93],\n",
       "         ...,\n",
       "         [108,   1,   1,   1],\n",
       "         [ 39,   1,   1,   1],\n",
       "         [  5,   1,   1,   1]]),\n",
       " 'toxic': tensor([0, 0, 0, 0]),\n",
       " 'severe_toxic': tensor([0, 0, 0, 0]),\n",
       " 'obscene': tensor([0, 0, 0, 0]),\n",
       " 'threat': tensor([0, 0, 0, 0]),\n",
       " 'insult': tensor([0, 0, 0, 0]),\n",
       " 'identity_hate': tensor([0, 0, 0, 0])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(list(train_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = list(train_iter)\n",
    "valid_dataloader = list(valid_iter)\n",
    "#test_dataloader = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTMBinary(nn.Module):\n",
    "    def __init__(self, vocab_length, hidden_dim, emb_dim=300, num_lstm_layers=1, num_linear_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_linear_layers = num_linear_layers\n",
    "        self.embedding = nn.Embedding(vocab_length, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers = num_lstm_layers)\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_linear_layers)]\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_dim, 6)  # 6 different binary labels\n",
    "        self.sigm = nn.Sigmoid()\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        preds = self.embedding(seq)\n",
    "        preds, _ = self.lstm(preds)\n",
    "        preds = preds[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            preds = self.sigm(layer(preds))\n",
    "        preds = self.output(preds)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "model = LSTMBinary(vocab_length=len(TEXT.vocab), hidden_dim=500, emb_dim=100, num_linear_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
      "INFO:__main__:Epoch: 1, Training Loss: 1.1390, Validation Loss: 5.9476\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.28s/it]\n",
      "INFO:__main__:Epoch: 2, Training Loss: 1.4952, Validation Loss: 5.5875\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n",
      "INFO:__main__:Epoch: 3, Training Loss: 1.9336, Validation Loss: 4.8728\n"
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 3\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        x = batch.comment_text\n",
    "        y = torch.cat([getattr(batch, label).unsqueeze(1) for label in labels], dim=1).float()\n",
    "        opt.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = loss_func(predictions, y)\n",
    "        loss.backward()\n",
    "        opt.step()     \n",
    "        running_loss += loss.data * x.size(0)\n",
    "    epoch_loss = running_loss / len(train)\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            x = batch.comment_text\n",
    "            y = torch.cat([getattr(batch, label).unsqueeze(1) for label in labels], dim=1).float()\n",
    "            predictions = model(x)\n",
    "            loss = loss_func(predictions, y)\n",
    "            valid_loss += loss.data * x.size(0)\n",
    "\n",
    "        valid_loss /= len(valid)\n",
    "    logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " [torchtext.data.batch.Batch of size 7]\n",
       " \t[.comment_text]:[torch.LongTensor of size 133x7]\n",
       " \t[.toxic]:[torch.LongTensor of size 7]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 7]\n",
       " \t[.obscene]:[torch.LongTensor of size 7]\n",
       " \t[.threat]:[torch.LongTensor of size 7]\n",
       " \t[.insult]:[torch.LongTensor of size 7]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 7], \n",
       " [torchtext.data.batch.Batch of size 7]\n",
       " \t[.comment_text]:[torch.LongTensor of size 48x7]\n",
       " \t[.toxic]:[torch.LongTensor of size 7]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 7]\n",
       " \t[.obscene]:[torch.LongTensor of size 7]\n",
       " \t[.threat]:[torch.LongTensor of size 7]\n",
       " \t[.insult]:[torch.LongTensor of size 7]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 7], \n",
       " [torchtext.data.batch.Batch of size 4]\n",
       " \t[.comment_text]:[torch.LongTensor of size 587x4]\n",
       " \t[.toxic]:[torch.LongTensor of size 4]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 4]\n",
       " \t[.obscene]:[torch.LongTensor of size 4]\n",
       " \t[.threat]:[torch.LongTensor of size 4]\n",
       " \t[.insult]:[torch.LongTensor of size 4]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 4], \n",
       " [torchtext.data.batch.Batch of size 7]\n",
       " \t[.comment_text]:[torch.LongTensor of size 17x7]\n",
       " \t[.toxic]:[torch.LongTensor of size 7]\n",
       " \t[.severe_toxic]:[torch.LongTensor of size 7]\n",
       " \t[.obscene]:[torch.LongTensor of size 7]\n",
       " \t[.threat]:[torch.LongTensor of size 7]\n",
       " \t[.insult]:[torch.LongTensor of size 7]\n",
       " \t[.identity_hate]:[torch.LongTensor of size 7]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = list(train_iter)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([587, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[2].comment_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      " more \n",
      " i <unk> n't make any <unk> <unk> on improvement - i <unk> if the <unk> <unk> should be later on , or a <unk> of \" \" <unk> of <unk> \" \"   <unk> think the references may need <unk> so that they are <unk> in the <unk> <unk> format <unk> <unk> format <unk> . i can do that later on , if no - one <unk> does first - if you have any <unk> for formatting <unk> on references or want to do it yourself please <unk> me know . \n",
      "\n",
      " there <unk> to be a <unk> on articles for review so i <unk> there may be a <unk> <unk> a <unk> <unk> up . it 's listed in the relevant <unk> <unk> wikipedia : <unk>   \"\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  <unk> are not always <unk> ! \n",
      "\n",
      " under <unk> it is <unk> that \" \" a <unk> always has <unk> <unk> <unk> . \" \" this <unk> is simply not <unk> ! <unk> to <unk> <unk> , \" \" the rather <unk> <unk> <unk> are by <unk> the most <unk> <unk> . \" \" <unk> someone really need to take a look at his <unk> and <unk> <unk> <unk> of it <unk> i <unk> see a <unk> <unk> of <unk> on this page . ( <unk> me i <unk> new at this and do <unk> want to edit anything ) \" <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "<unk> if the <unk> ' <unk> ' was <unk> to you . <unk> , i 'm not <unk> to write anything in the <unk> they would <unk> on me for vandalism ) , i 'm merely <unk> that it be more <unk> so one can use it for <unk> as a <unk> . i have been to the <unk> breeding page but it 's <unk> a <unk> . it <unk> to ' <unk> breeding ' which is a <unk> <unk> article that <unk> you no info . there must be someone around with <unk> in <unk> ? <unk> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "do n't <unk> to <unk> you \n",
      "\n",
      " i see that you <unk> writing <unk> regarding removing anything <unk> <unk> and if you do oh well but if not and you can <unk> discuss this with me then even <unk> . \n",
      "\n",
      " i <unk> like to ask you to take a <unk> look at the <unk> <unk> <unk> catagory and the men listed in it , <unk> <unk> men belong together in some catagory . is there anything that you think we can do with the catagory <unk> <unk> it ? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "hey ... what is it <unk> \n",
      " <unk> <unk> talk . \n",
      " what is it ... an <unk> <unk> of some wp <unk> ... who are good at <unk> , <unk> - <unk> <unk> who <unk> up any one who <unk> them questions <unk> their <unk> - <unk> and <unk> ( <unk> at wp ? \n",
      "\n",
      " ask <unk> to <unk> up his <unk> than issue me <unk> warnings ... <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "\" \n",
      "\n",
      "  regarding your recent edits \n",
      "\n",
      " <unk> again , please read wp : <unk> before <unk> any more <unk> articles .   your edits are simply not good , with <unk> too <unk> <unk> <unk> and very <unk> writing .   please stop before you do <unk> <unk> . <unk> ' <unk> \" <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      " NEW SENTENCE \n",
      "\n",
      "explanation \n",
      " why the edits made under my <unk> <unk> <unk> <unk> were reverted ? they were n't <unk> , just <unk> on some <unk> after i <unk> at new <unk> <unk> <unk> . and please do n't <unk> the template from the talk page since i 'm <unk> <unk> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(l[0].comment_text[1])):\n",
    "    print(\"\\n NEW SENTENCE \\n\")\n",
    "    print(' '.join([TEXT.vocab.itos[x] for x in l[0].comment_text[:,i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
