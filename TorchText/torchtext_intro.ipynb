{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext tutorial\n",
    "\n",
    "\n",
    "Made by [Andrea Sottana](http://github.com/AndreaSottana) and adapted from the following [tutorial](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)\n",
    "\n",
    "This notebook shows how you can use your own custom dataset starting from the raw form (for example a .csv file) with the `torch` library, to easily do some cleaning and preprocessing (such as tokenization), build a vocabulary, and ultimately create an `Iterator` or `BucketIterator` object that you can feed it into a neural network built using the `torch` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset, Iterator, BucketIterator\n",
    "from torchtext import datasets\n",
    "import torch\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=20)  # shows info level loggings (we'll use this to print the loss during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data `Field` specifies how you want a certain field to be processed, for example whether you want to tokenize it, lower case the text etc. In our case we have one text field, and one label field for the sentiment analysis, which we represent with an integer, where 0 is negative, 1 is neutral and 2 is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize='spacy', lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>numerical_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It 's a lovely film with lovely performances b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No one goes unindicted here , which is probabl...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And if you 're not nearly moved to tears by a ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A warm , funny , engaging film .</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Uses sharp humor and insight into human nature...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>it seems to me the film is about the art of ri...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>It 's just disappointingly superficial -- a mo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>The title not only describes its main characte...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>Sometimes it feels as if it might have been ma...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>Schaeffer has to find some hook on which to ha...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels  \\\n",
       "0     It 's a lovely film with lovely performances b...  positive   \n",
       "1     No one goes unindicted here , which is probabl...   neutral   \n",
       "2     And if you 're not nearly moved to tears by a ...  positive   \n",
       "3                      A warm , funny , engaging film .  positive   \n",
       "4     Uses sharp humor and insight into human nature...  positive   \n",
       "...                                                 ...       ...   \n",
       "1096  it seems to me the film is about the art of ri...  negative   \n",
       "1097  It 's just disappointingly superficial -- a mo...  negative   \n",
       "1098  The title not only describes its main characte...  negative   \n",
       "1099  Sometimes it feels as if it might have been ma...   neutral   \n",
       "1100  Schaeffer has to find some hook on which to ha...  negative   \n",
       "\n",
       "      numerical_labels  \n",
       "0                    2  \n",
       "1                    1  \n",
       "2                    2  \n",
       "3                    2  \n",
       "4                    2  \n",
       "...                ...  \n",
       "1096                 0  \n",
       "1097                 0  \n",
       "1098                 0  \n",
       "1099                 1  \n",
       "1100                 0  \n",
       "\n",
       "[1101 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember to source the .envrc file in the terminal before launching this notebook to \n",
    "# ensure it can use the environment variables correctly.\n",
    "\n",
    "folder = os.path.join(os.getenv('DATA_DIR'), 'movie_review_dataset')\n",
    "train_dataset = pd.read_csv(os.path.join(folder, 'train_dataset.csv'))\n",
    "valid_dataset = pd.read_csv(os.path.join(folder, 'valid_dataset.csv'))\n",
    "test_dataset = pd.read_csv(os.path.join(folder, 'test_dataset.csv'))\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells the field what data to work on. Note that the fields we pass in must be in the same order as the columns in our csv file. For the columns we don't use, we pass in a tuple where the field element is None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', <torchtext.data.field.Field at 0x13694beb8>),\n",
       " ('labels', None),\n",
       " ('numerical_labels', <torchtext.data.field.Field at 0x13861f2e8>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [(\"text\", TEXT), (\"labels\", None), (\"numerical_labels\", LABEL)]\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TabularDataset` is one of the built-in Datasets in torchtext that handle common data formats; this one is good for .csv files. Other datasets, such as `LanguageModelingDataset` or `TranslationDataset`, are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TabularDataset.splits(\n",
    "    path = folder,\n",
    "    train = 'train_dataset.csv',\n",
    "    validation = 'valid_dataset.csv',\n",
    "    test = 'test_dataset.csv',\n",
    "    format = 'csv',\n",
    "    skip_header = True,\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x13a95eb70> \n",
      "\n",
      "{'text': ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '`', '`', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', '-', 'claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], 'numerical_labels': '2'} \n",
      "\n",
      "dict_keys(['text', 'numerical_labels']) \n",
      "\n",
      "['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '`', '`', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', '-', 'claud', 'van', 'damme', 'or', 'steven', 'segal', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train[0], '\\n')\n",
    "\n",
    "# This is an Example object. The Example object bundles the attributes of a single data \n",
    "# point together. We also see that the text has already been tokenized for us, but has \n",
    "# not yet been converted to integers. \n",
    "\n",
    "print(train[0].__dict__, '\\n')\n",
    "print(train[0].__dict__.keys(), '\\n')\n",
    "print(train[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is constructing the mapping from words to ids. The vocabulary is normally built on the training set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext has its own class called Vocab for handling the vocabulary. The Vocab class holds a mapping from word to id in its `stoi` attribute and a reverse mapping in its `itos` attribute. Words that are not included in the vocabulary will be converted into `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[12])\n",
    "print(TEXT.vocab.stoi['it'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create an Iterator to pass the data to our model.  \n",
    "\n",
    "In torchvision and PyTorch, the processing and batching of data is handled by DataLoaders. For some reason, torchtext has renamed the objects that do the exact same thing to Iterators. The basic functionality is the same, but Iterators, as we will see, have some convenient functionality that is unique to NLP.  \n",
    "\n",
    "We also need to tell the BucketIterator what attribute you want to bucket the data on. In our case, we want to bucket based on the lengths of the comment_text field, so we pass that in as a keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(64, 64, 64),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative would be to use a standard iterator for the test set, given we do not need to shuffle the data since we'll be outputting the predictions at the end of training.  \n",
    "The code would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "    (train, valid),\n",
    "    batch_sizes=(64, 64),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")\n",
    "\n",
    "test_iter = Iterator(\n",
    "    test, batch_size=64, device=device, sort=False, sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert the batch to a tuple in the form (x, y) where x is the independent variable (the input to the model) and y is the dependent variable (the supervision data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, iterator, feature, label: list):\n",
    "        self.iterator = iterator\n",
    "        self.feature = feature\n",
    "        self.label = label\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.iterator:\n",
    "            x = getattr(batch, self.feature)  # we assume only one input in this wrapper\n",
    "            if self.label is not None:  # we will concatenate y into a single tensor\n",
    "                y = torch.cat(\n",
    "                    [getattr(batch, lab).unsqueeze(1) for lab in self.label], dim=1\n",
    "                    # this is just in case we have multiple labels\n",
    "                ).float()\n",
    "            else:\n",
    "                y = torch.zeros(1)\n",
    "            yield (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # This is so the iteration will occur over the indices from zero to len-1.\n",
    "        return len(self.iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above works if we have multiple labels. If we only have one label, we can simplify the code as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# works only for single label\n",
    "\n",
    "class BatchWrapperSingleLabel:\n",
    "    def __init__(self, iterator, feature, label: str):\n",
    "        self.iterator = iterator\n",
    "        self.feature = feature\n",
    "        self.label = label\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.iterator:\n",
    "            x = getattr(batch, self.feature)  # we assume only one input in this wrapper\n",
    "            if self.label is not None: \n",
    "                y = getattr(batch, self.label).unsqueeze(1).float()\n",
    "            else:\n",
    "                y = torch.zeros(1)\n",
    "            yield (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # This is so the iteration will occur over the indices from zero to len-1.\n",
    "        return len(self.iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code to play with instances of the `BatchWrapper` class to see what it's like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14, 64]), torch.Size([64, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = BatchWrapper(train_iter, \"text\", [\"numerical_labels\"])\n",
    "valid_dataloader = BatchWrapper(valid_iter, \"text\", [\"numerical_labels\"])\n",
    "test_dataloader = BatchWrapper(test_iter, \"text\", None)  # no labels given for testing\n",
    "\n",
    "it = next(train_dataloader.__iter__())\n",
    "it[0].shape, it[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_iter)[0].numerical_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dataset': <torchtext.data.dataset.TabularDataset at 0x13a95eac8>,\n",
       " 'fields': dict_keys(['text', 'labels', 'numerical_labels']),\n",
       " 'input_fields': ['text', 'numerical_labels'],\n",
       " 'target_fields': [],\n",
       " 'text': tensor([[  55,    3,    3,  ...,   55, 2148,   21],\n",
       "         [  10,   20,   20,  ...,   38,    5, 3016],\n",
       "         [  22,   10, 1061,  ...,   15,  177, 1886],\n",
       "         ...,\n",
       "         [   5,    1,    1,  ...,   28,    1,    1],\n",
       "         [1832,    1,    1,  ...,  940,    1,    1],\n",
       "         [   2,    1,    1,  ...,    2,    1,    1]]),\n",
       " 'numerical_labels': tensor([1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0,\n",
       "         1, 0, 2, 0, 0, 0, 1, 0, 2, 2, 2, 0, 0, 2, 1, 1, 0, 0, 2, 2, 2, 2, 0, 2,\n",
       "         0, 2, 2, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 1])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(list(test_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "<class 'list'>\n",
      "<generator object BatchWrapper.__iter__ at 0x13fe59c50>\n"
     ]
    }
   ],
   "source": [
    "iterator = train_dataloader.__iter__()\n",
    "print(len(list(iterator)))\n",
    "print(type(list(iterator)))\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to re-define them as we've already used the generator above\n",
    "\n",
    "train_dataloader = BatchWrapper(train_iter, \"text\", [\"numerical_labels\"])\n",
    "valid_dataloader = BatchWrapper(valid_iter, \"text\", [\"numerical_labels\"])\n",
    "test_dataloader = BatchWrapper(test_iter, \"text\", None)  # no labels given for testing\n",
    "iterator = train_dataloader.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'tuple'>\n",
      "(tensor([[ 3422,  2529,  1608,    22,    54,  8374,  3927,     5,  7405,     5,\n",
      "            12,  3581,    12,   446,   115,    29,    31,    46,   927,    55,\n",
      "            22,   676,     5,    46,  8212,   321,   538,   117,    46,   176,\n",
      "            12,    75,    22,    29,  2577,  2497,     3,  3445,  8150,    16,\n",
      "            54,    16,   100,   308,   933,     5,  2029,    62,   437,    62,\n",
      "            22, 13707,   704,   859,  8050,    22,    29,    29,    54,  3869,\n",
      "          1254,     6, 10188,    12],\n",
      "        [   16,    16,     6,  1861,    11,  1123,     6,    20,     6,  1252,\n",
      "            11,     4,    11,   176,    19,   337,  1675,    37,     6,    10,\n",
      "           694,     6,    61,    37,     9,    10,    16,   193,    65, 15067,\n",
      "            11,     4,  4953,  6830,  2689,     8,   412,  3094,  3247,    46,\n",
      "            22,    64,    46,   185,     6,  1114,   383,    37,     6,   828,\n",
      "          1123,  6269,   757,   191,     6,  1255,  4330, 13721,    11,     6,\n",
      "          2080,    12,    33,    11],\n",
      "        [ 1127,   757,   343,   303,   443,  1457,   755,     8, 10890,  3210,\n",
      "          4340,   581,   282,   834,   426, 11365,    20,    12,    75,   685,\n",
      "          7779,   801,   191,    12,  8177,   114,  1658,    75,    27,   560,\n",
      "          4720,   155,    17,   295,   621,  7391, 15318,   420,  5336,    86,\n",
      "          2328,     2,  7648,    52,   227,    68,    12,  3928,  1284,    96,\n",
      "          1687, 14559,   487,  2072,   375,   303,  1114,  1137,   443, 10086,\n",
      "          1650,    10,    34,    75],\n",
      "        [    2,     2,     2,     2,   107,     2,     2,   910,     2,     2,\n",
      "             2,     2,     2,     2,     2,    29,     2,     2,     2,   151,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,   188,     2,   188,     2,\n",
      "             2,     2,   107,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,   188,     2,     2,     2,     2,     2,     2,   107,     2,\n",
      "           107,     2,  3092,     2]]), tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.]]))\n"
     ]
    }
   ],
   "source": [
    "next_ = next(iterator)\n",
    "print(len(next_))\n",
    "print(type(next_))\n",
    "print(next_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "print(next_[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 64])\n",
      "torch.Size([28, 64])\n",
      "torch.Size([9, 64])\n"
     ]
    }
   ],
   "source": [
    "print(next(iterator)[0].shape)  # lenght of sentence, batch size\n",
    "print(next(iterator)[0].shape)\n",
    "print(next(iterator)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating the instances of the `BatchWrapperSingleLabel` class, which we'll then use to feed into our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = BatchWrapperSingleLabel(train_iter, \"text\", \"numerical_labels\")\n",
    "valid_dataloader = BatchWrapperSingleLabel(valid_iter, \"text\", \"numerical_labels\")\n",
    "test_dataloader = BatchWrapperSingleLabel(test_iter, \"text\", None)  # no labels given for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is, finally, our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class SimpleLSTMBaseline(nn.Module):\n",
    "    def __init__(self, vocab_length, hidden_dim, emb_dim=300, num_lstm_layers=1, num_linear_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_length, emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=num_lstm_layers)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear_layers):  # - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 1)  # nn.Linear(hidden_dim, 6)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "            preds = self.predictor(feature)\n",
    "        return preds\n",
    "\n",
    "\n",
    "model = SimpleLSTMBaseline(vocab_length=len(TEXT.vocab), hidden_dim=500, emb_dim=100, num_linear_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:19<00:00,  1.68it/s]\n",
      "INFO:__main__:Epoch: 1, Training Loss: -73641492480.0000, Validation Loss: -526479753216.0000\n",
      "100%|██████████| 134/134 [01:06<00:00,  2.01it/s]\n",
      "INFO:__main__:Epoch: 2, Training Loss: -5292373835776.0000, Validation Loss: -17754839056384.0000\n"
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 2\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x, y in tqdm(train_dataloader): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_func(y, preds)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.data * x.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train)\n",
    "\n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dataloader:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(y, preds)\n",
    "        # val_loss += loss.data[0] * x.size(0)\n",
    "        val_loss += loss.data * x.size(0)\n",
    "\n",
    "    val_loss /= len(valid)\n",
    "    logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
